{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Workflow\n",
    "\n",
    "This notebook demonstrates the research [workflow](https://langchain-ai.github.io/langgraph/tutorials/workflows/) that creates comprehensive reports through a series of focused steps. The system:\n",
    "\n",
    "1. Uses a **graph workflow** with specialized nodes for each report creation stage\n",
    "2. Enables user **feedback and approval** at critical planning points \n",
    "3. Produces a well-structured report with introduction, researched body sections, and conclusion\n",
    "\n",
    "## From repo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\62687\\Desktop\\个人\\实习相关\\标准化研究院实习\\open_deep_research\\src\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile the Graph-Based Research Workflow\n",
    "\n",
    "The next step is to compile the LangGraph workflow that orchestrates the report creation process. This defines the sequence of operations and decision points in the research pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0.15\n",
      "builder compiling ...\n",
      "builder compiled.\n",
      "deepseek-chat\n",
      "deepseek\n"
     ]
    }
   ],
   "source": [
    "# Import required modules and initialize the builder from open_deep_research\n",
    "import uuid \n",
    "import os, getpass\n",
    "import open_deep_research   \n",
    "print(open_deep_research.__version__) \n",
    "from IPython.display import Image, display, Markdown\n",
    "from langgraph.types import Command\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from open_deep_research.graph import builder\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Create a memory-based checkpointer and compile the graph\n",
    "# This enables state persistence and tracking throughout the workflow execution\n",
    "\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(checkpointer=memory)\n",
    "load_dotenv(\"../.env\")\n",
    "print(os.environ[\"PLANNER_MODEL\"])\n",
    "print(os.environ[\"PLANNER_PROVIDER\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "builder compiling ...\n",
      "builder compiled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'open_deep_research.graph' from 'c:\\\\Users\\\\62687\\\\Desktop\\\\个人\\\\实习相关\\\\标准化研究院实习\\\\open_deep_research\\\\src\\\\open_deep_research\\\\graph.py'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import open_deep_research\n",
    "import open_deep_research.prompts\n",
    "import open_deep_research.configuration\n",
    "import open_deep_research.utils\n",
    "import open_deep_research.graph\n",
    "from importlib import reload\n",
    "\n",
    "reload(open_deep_research) \n",
    "reload(open_deep_research.prompts)\n",
    "reload(open_deep_research.configuration)\n",
    "reload(open_deep_research.utils) \n",
    "reload(open_deep_research.graph) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the graph structure\n",
    "# This shows the nodes and edges in the research workflow\n",
    "\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入字符串的 Token 数: 65612, 最大限制: 15000\n",
      "输入字符串的 Token 数: 65612, 超过最大限制 15000，已缩减字符串长度到 15878 字符\n",
      "Interrupt value: 请对以下报告框架提供反馈。\n",
      "\n",
      "第1章 产品质量发展现状\n",
      "\t第1节 总体概况\n",
      "\t\t概要: 结合2024年的数据和政策分析湖南省制造业产品质量合格率的整体情况。\n",
      "\t\t是否需要联网搜索: 是\n",
      "\t第2节 地区概况\n",
      "\t\t概要: 分析湖南省各地区制造业产品质量合格率的总体情况，结合政策背景。\n",
      "\t\t是否需要联网搜索: 是\n",
      "\t第3节 行业概况\n",
      "\t\t概要: 分析湖南省各行业制造业产品质量合格率的总体情况，结合政策背景。\n",
      "\t\t是否需要联网搜索: 是\n",
      "第2章 地区产品质量状况\n",
      "\t第1节 长沙市\n",
      "\t\t概要: 聚焦长沙市制造业产品质量合格率的具体情况，分析其发展现状。\n",
      "\t\t是否需要联网搜索: 是\n",
      "\t第2节 株洲市\n",
      "\t\t概要: 聚焦株洲市制造业产品质量合格率的具体情况，分析其发展现状。\n",
      "\t\t是否需要联网搜索: 是\n",
      "\t第3节 湘潭市\n",
      "\t\t概要: 聚焦湘潭市制造业产品质量合格率的具体情况，分析其发展现状。\n",
      "\t\t是否需要联网搜索: 是\n",
      "第3章 行业产品质量状况\n",
      "\t第1节 装备制造类行业\n",
      "\t\t概要: 分析装备制造类行业产品质量合格率的具体情况，探讨其发展现状。\n",
      "\t\t是否需要联网搜索: 是\n",
      "\t第2节 资源加工类行业\n",
      "\t\t概要: 分析资源加工类行业产品质量合格率的具体情况，探讨其发展现状。\n",
      "\t\t是否需要联网搜索: 是\n",
      "\t第3节 食药烟酒类行业\n",
      "\t\t概要: 分析食药烟酒类行业产品质量合格率的具体情况，探讨其发展现状。\n",
      "\t\t是否需要联网搜索: 是\n",
      "第4章 问题分析\n",
      "\t第1节 支柱产业承压下滑，质量管控短板突出\n",
      "\t\t概要: 分析当前支柱产业质量管控中存在的问题和挑战。\n",
      "\t\t是否需要联网搜索: 否\n",
      "\t第2节 区域质量发展失衡，湘西地区水平落后\n",
      "\t\t概要: 探讨湖南省各地区质量发展不平衡的问题。\n",
      "\t\t是否需要联网搜索: 否\n",
      "\t第3节 新兴产业波动加剧，质量稳定有待加强\n",
      "\t\t概要: 分析新兴产业产品质量波动的原因及改进方向。\n",
      "\t\t是否需要联网搜索: 否\n",
      "\t第4节 小型企业基础薄弱，存在质量安全隐患\n",
      "\t\t概要: 探讨小型企业在质量安全方面的薄弱环节。\n",
      "\t\t是否需要联网搜索: 否\n",
      "第5章 政策建议\n",
      "\t第1节 优化完善市场监管手段措施，防范化解重点领域质量风险问题\n",
      "\t\t概要: 提出针对市场监管和质量风险防范的政策建议。\n",
      "\t\t是否需要联网搜索: 否\n",
      "\t第2节 深入推进质量强企强链强县建设，提升产业链韧性及区域发展协同性\n",
      "\t\t概要: 建议通过强企强链强县建设提升产业链韧性和区域协同性。\n",
      "\t\t是否需要联网搜索: 否\n",
      "\t第3节 构建高水平质量基础设施建设体系，发挥服务质量提升的支撑保障作用\n",
      "\t\t概要: 提出构建高质量基础设施体系的建议，以支撑质量提升。\n",
      "\t\t是否需要联网搜索: 否\n",
      "\n",
      "\n",
      "    \n",
      "该报告框架是否符合您的需求？\n",
      "输入'true'以确认采用该报告框架。\n",
      "或提供修改意见以重新生成报告框架：\n"
     ]
    }
   ],
   "source": [
    "# Define report structure template and configure the research workflow\n",
    "# This sets parameters for models, search tools, and report organization\n",
    "\n",
    "# REPORT_STRUCTURE = \"\"\"Use this structure to create a report on the user-provided topic:\n",
    "\n",
    "# 1. Introduction (no research needed)\n",
    "#    - Brief overview of the topic area\n",
    "\n",
    "# 2. Main Body Sections:\n",
    "#    - Each section should focus on a sub-topic of the user-provided topic\n",
    "   \n",
    "# 3. Conclusion\n",
    "#    - Aim for 1 structural element (either a list of table) that distills the main body sections \n",
    "#    - Provide a concise summary of the report\"\"\"\n",
    "# topic = \"Overview of Model Context Protocol (MCP), an Anthropic‑backed open standard for integrating external context and tools with LLMs. Give an architectural overview for developers, tell me about interesting MCP servers, and compare to google Agent2Agent (A2A) protocol.\"\n",
    "\n",
    "REPORT_STRUCTURE = \"\"\" \n",
    "第一章、产品质量发展现状\n",
    "   第一节、总体概况：结合当年的数据和政策分析整体情况\n",
    "   第二节、地区概况：结合当年的数据和政策总体分析各地区的概况\n",
    "   第三节、行业概况：结合当年的数据和政策总体分析各行业的概况\n",
    "第二章、地区产品质量状况：包含若干节（对应于第一章第二节中列举的地区），每一节都聚焦于一个特定地区的产品质量状况，分析该地区的产品质量发展现状，若报告范围为全国则列举所有省份，若报告范围为某省份则列举所有地级市\n",
    "第三章、行业产品质量状况：包含3-5节，每一节都聚焦于一个特定行业的产品质量状况，分析该行业的产品质量发展现状，比如装备制造类行业、资源加工类行业、食药烟酒类行业以及消费品工业制造业等等\n",
    "第四章、问题分析：包括3-5节，结合前面章节的内容，分析当前产品质量发展中存在的问题和挑战\n",
    "第五章、政策建议：包括3-5节，提出针对第四章中分析出的问题的解决方案和建议\n",
    "\"\"\"\n",
    "topic = \"湖南省制造业产品质量合格率分析报告（2024年）\"\n",
    "\n",
    "TAVILY_API_KEY = os.environ.get(\"TAVILY_API_KEY\")\n",
    "DEEPSEEK_API_KEY = os.environ.get(\"DEEPSEEK_API_KEY\")\n",
    "DEEPSEEK_MODEL = os.environ.get(\"PLANNER_MODEL\", \"deepseek-chat\")\n",
    "PROVIDER = os.environ.get(\"WRITER_PROVIDER\", \"deepseek\")\n",
    "BASE_URL = 'https://api.deepseek.com/v1'\n",
    "thread = {\"configurable\": {\"thread_id\": str(uuid.uuid4()),\n",
    "                           \"search_api\": \"tavily\",\n",
    "                           \"search_api_config\": {\"api_key\": TAVILY_API_KEY},\n",
    "                           \"planner_provider\": PROVIDER,\n",
    "                           \"planner_model\": DEEPSEEK_MODEL,\n",
    "                           \"planner_model_kwargs\": {\n",
    "                              \"api_key\": DEEPSEEK_API_KEY,\n",
    "                              \"base_url\": BASE_URL\n",
    "                           },\n",
    "                           \"writer_provider\": PROVIDER,\n",
    "                           \"writer_model\": DEEPSEEK_MODEL,\n",
    "                           \"writer_model_kwargs\": {\n",
    "                              \"api_key\": DEEPSEEK_API_KEY,\n",
    "                              \"base_url\": BASE_URL\n",
    "                           },\n",
    "                           \"summarization_model_provider\": PROVIDER,\n",
    "                           \"summarization_model_model\": DEEPSEEK_MODEL,\n",
    "                           \"summarization_model_kwargs\": {\n",
    "                              \"api_key\": DEEPSEEK_API_KEY,\n",
    "                              \"base_url\": BASE_URL\n",
    "                           },      \n",
    "                           \"max_search_depth\": 2,\n",
    "                           \"report_structure\": REPORT_STRUCTURE,\n",
    "                           }}\n",
    "\n",
    "# Define research topic about Model Context Protocol\n",
    "\n",
    "\n",
    "# Run the graph workflow until first interruption (waiting for user feedback)\n",
    "async for event in graph.astream({\"topic\":topic,}, thread, stream_mode=\"updates\"):\n",
    "   if '__interrupt__' in event:\n",
    "      interrupt_value = event['__interrupt__'][0].value\n",
    "      #   display(Markdown(interrupt_value))\n",
    "      print(\"Interrupt value:\", interrupt_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Feedback Phase\n",
    "\n",
    "* This allows for providing directed feedback on the initial report plan\n",
    "* The user can review the proposed report structure and provide specific guidance\n",
    "* The system will incorporate this feedback into the final report plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入字符串的 Token 数: 65549, 最大限制: 15000\n",
      "输入字符串的 Token 数: 65549, 超过最大限制 15000，已缩减字符串长度到 15904 字符\n",
      "Interrupt value: 请对以下报告框架提供反馈。\n",
      "\n",
      "第1章 产品质量发展现状\n",
      "\t第1节 总体概况\n",
      "\t\t概要: 结合2024年的数据和政策分析湖南省制造业产品质量合格率的整体情况。\n",
      "\t\t是否需要联网搜索: 是\n",
      "\t第2节 地区概况\n",
      "\t\t概要: 分析湖南省各地区制造业产品质量合格率的总体情况，包括主要数据和政策背景。\n",
      "\t\t是否需要联网搜索: 是\n",
      "\t第3节 行业概况\n",
      "\t\t概要: 分析湖南省各行业制造业产品质量合格率的总体情况，包括主要数据和政策背景。\n",
      "\t\t是否需要联网搜索: 是\n",
      "第2章 地区产品质量状况\n",
      "\t第1节 长沙市产品质量状况\n",
      "\t\t概要: 分析长沙市制造业产品质量合格率的具体情况，包括数据、政策及典型案例。\n",
      "\t\t是否需要联网搜索: 是\n",
      "\t第2节 株洲市产品质量状况\n",
      "\t\t概要: 分析株洲市制造业产品质量合格率的具体情况，包括数据、政策及典型案例。\n",
      "\t\t是否需要联网搜索: 是\n",
      "\t第3节 湘潭市产品质量状况\n",
      "\t\t概要: 分析湘潭市制造业产品质量合格率的具体情况，包括数据、政策及典型案例。\n",
      "\t\t是否需要联网搜索: 是\n",
      "\t第4节 衡阳市产品质量状况\n",
      "\t\t概要: 分析衡阳市制造业产品质量合格率的具体情况，包括数据、政策及典型案例。\n",
      "\t\t是否需要联网搜索: 是\n",
      "\t第5节 邵阳市产品质量状况\n",
      "\t\t概要: 分析邵阳市制造业产品质量合格率的具体情况，包括数据、政策及典型案例。\n",
      "\t\t是否需要联网搜索: 是\n",
      "\t第6节 岳阳市产品质量状况\n",
      "\t\t概要: 分析岳阳市制造业产品质量合格率的具体情况，包括数据、政策及典型案例。\n",
      "\t\t是否需要联网搜索: 是\n",
      "\t第7节 常德市产品质量状况\n",
      "\t\t概要: 分析常德市制造业产品质量合格率的具体情况，包括数据、政策及典型案例。\n",
      "\t\t是否需要联网搜索: 是\n",
      "\t第8节 张家界市产品质量状况\n",
      "\t\t概要: 分析张家界市制造业产品质量合格率的具体情况，包括数据、政策及典型案例。\n",
      "\t\t是否需要联网搜索: 是\n",
      "\t第9节 益阳市产品质量状况\n",
      "\t\t概要: 分析益阳市制造业产品质量合格率的具体情况，包括数据、政策及典型案例。\n",
      "\t\t是否需要联网搜索: 是\n",
      "\t第10节 郴州市产品质量状况\n",
      "\t\t概要: 分析郴州市制造业产品质量合格率的具体情况，包括数据、政策及典型案例。\n",
      "\t\t是否需要联网搜索: 是\n",
      "\t第11节 永州市产品质量状况\n",
      "\t\t概要: 分析永州市制造业产品质量合格率的具体情况，包括数据、政策及典型案例。\n",
      "\t\t是否需要联网搜索: 是\n",
      "\t第12节 怀化市产品质量状况\n",
      "\t\t概要: 分析怀化市制造业产品质量合格率的具体情况，包括数据、政策及典型案例。\n",
      "\t\t是否需要联网搜索: 是\n",
      "\t第13节 娄底市产品质量状况\n",
      "\t\t概要: 分析娄底市制造业产品质量合格率的具体情况，包括数据、政策及典型案例。\n",
      "\t\t是否需要联网搜索: 是\n",
      "\t第14节 湘西土家族苗族自治州产品质量状况\n",
      "\t\t概要: 分析湘西土家族苗族自治州制造业产品质量合格率的具体情况，包括数据、政策及典型案例。\n",
      "\t\t是否需要联网搜索: 是\n",
      "第3章 行业产品质量状况\n",
      "\t第1节 装备制造类行业产品质量状况\n",
      "\t\t概要: 分析湖南省装备制造类行业产品质量合格率的具体情况，包括数据、政策及典型案例。\n",
      "\t\t是否需要联网搜索: 是\n",
      "\t第2节 资源加工类行业产品质量状况\n",
      "\t\t概要: 分析湖南省资源加工类行业产品质量合格率的具体情况，包括数据、政策及典型案例。\n",
      "\t\t是否需要联网搜索: 是\n",
      "\t第3节 食药烟酒类行业产品质量状况\n",
      "\t\t概要: 分析湖南省食药烟酒类行业产品质量合格率的具体情况，包括数据、政策及典型案例。\n",
      "\t\t是否需要联网搜索: 是\n",
      "\t第4节 消费品工业制造业产品质量状况\n",
      "\t\t概要: 分析湖南省消费品工业制造业产品质量合格率的具体情况，包括数据、政策及典型案例。\n",
      "\t\t是否需要联网搜索: 是\n",
      "第4章 问题分析\n",
      "\t第1节 支柱产业承压下滑，质量管控短板突出\n",
      "\t\t概要: 分析湖南省制造业支柱产业在质量管控方面存在的问题和挑战。\n",
      "\t\t是否需要联网搜索: 是\n",
      "\t第2节 区域质量发展失衡，湘西地区水平落后\n",
      "\t\t概要: 分析湖南省各地区制造业质量发展不平衡的问题，特别是湘西地区的落后情况。\n",
      "\t\t是否需要联网搜索: 是\n",
      "\t第3节 新兴产业波动加剧，质量稳定有待加强\n",
      "\t\t概要: 分析湖南省新兴产业在质量稳定性方面的问题和挑战。\n",
      "\t\t是否需要联网搜索: 是\n",
      "\t第4节 小型企业基础薄弱，存在质量安全隐患\n",
      "\t\t概要: 分析湖南省小型制造业企业在质量安全方面的薄弱环节和隐患。\n",
      "\t\t是否需要联网搜索: 是\n",
      "第5章 政策建议\n",
      "\t第1节 优化完善市场监管手段措施，防范化解重点领域质量风险问题\n",
      "\t\t概要: 提出优化市场监管手段的建议，以防范和化解重点领域的质量风险。\n",
      "\t\t是否需要联网搜索: 否\n",
      "\t第2节 深入推进质量强企强链强县建设，提升产业链韧性及区域发展协同性\n",
      "\t\t概要: 提出加强质量强企、强链、强县建设的建议，以提升产业链韧性和区域协同发展。\n",
      "\t\t是否需要联网搜索: 否\n",
      "\t第3节 构建高水平质量基础设施建设体系，发挥服务质量提升的支撑保障作用\n",
      "\t\t概要: 提出构建高水平质量基础设施体系的建议，以支撑和保障服务质量提升。\n",
      "\t\t是否需要联网搜索: 否\n",
      "\n",
      "\n",
      "    \n",
      "该报告框架是否符合您的需求？\n",
      "输入'true'以确认采用该报告框架。\n",
      "或提供修改意见以重新生成报告框架：\n"
     ]
    }
   ],
   "source": [
    "# Submit feedback on the report plan\n",
    "# The system will continue execution with the updated requirements\n",
    "\n",
    "# Provide specific feedback to focus and refine the report structure\n",
    "async for event in graph.astream(Command(resume=\"第二章的地区不够多，把湖南省的地级市都包括，每一个地级市一节\"), thread, stream_mode=\"updates\"):\n",
    "    if '__interrupt__' in event:\n",
    "        interrupt_value = event['__interrupt__'][0].value\n",
    "        print(\"Interrupt value:\", interrupt_value)\n",
    "        # display(Markdown(interrupt_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Approval Phase\n",
    "* After incorporating feedback, approve the plan to start content generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AsyncTavilyClient' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Approve the final plan and execute the report generation\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# This triggers the research and writing phases for all sections\u001b[39;00m\n\u001b[32m      3\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# 3. Create introduction and conclusion\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# 4. Compile the final report\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m graph.astream(Command(resume=\u001b[38;5;28;01mTrue\u001b[39;00m), thread, stream_mode=\u001b[33m\"\u001b[39m\u001b[33mupdates\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(event)\n\u001b[32m     12\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\62687\\Miniconda3\\envs\\deep_research\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2767\u001b[39m, in \u001b[36mPregel.astream\u001b[39m\u001b[34m(self, input, config, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2765\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m loop.amatch_cached_writes():\n\u001b[32m   2766\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2767\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.atick(\n\u001b[32m   2768\u001b[39m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop.tasks.values() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.writes],\n\u001b[32m   2769\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   2770\u001b[39m     get_waiter=get_waiter,\n\u001b[32m   2771\u001b[39m     schedule_task=loop.aaccept_push,\n\u001b[32m   2772\u001b[39m ):\n\u001b[32m   2773\u001b[39m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   2774\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m _output(\n\u001b[32m   2775\u001b[39m         stream_mode,\n\u001b[32m   2776\u001b[39m         print_mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2779\u001b[39m         asyncio.QueueEmpty,\n\u001b[32m   2780\u001b[39m     ):\n\u001b[32m   2781\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\62687\\Miniconda3\\envs\\deep_research\\Lib\\site-packages\\langgraph\\pregel\\runner.py:401\u001b[39m, in \u001b[36mPregelRunner.atick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     _panic_or_proceed(\n\u001b[32m    402\u001b[39m         futures.done.union(f \u001b[38;5;28;01mfor\u001b[39;00m f, t \u001b[38;5;129;01min\u001b[39;00m futures.items() \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    403\u001b[39m         timeout_exc_cls=asyncio.TimeoutError,\n\u001b[32m    404\u001b[39m         panic=reraise,\n\u001b[32m    405\u001b[39m     )\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    407\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tb := exc.__traceback__:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\62687\\Miniconda3\\envs\\deep_research\\Lib\\site-packages\\langgraph\\pregel\\runner.py:511\u001b[39m, in \u001b[36m_panic_or_proceed\u001b[39m\u001b[34m(futs, timeout_exc_cls, panic)\u001b[39m\n\u001b[32m    509\u001b[39m                 interrupts.append(exc)\n\u001b[32m    510\u001b[39m             \u001b[38;5;28;01melif\u001b[39;00m fut \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m SKIP_RERAISE_SET:\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    512\u001b[39m \u001b[38;5;66;03m# raise combined interrupts\u001b[39;00m\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m interrupts:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\62687\\Miniconda3\\envs\\deep_research\\Lib\\site-packages\\langgraph\\pregel\\retry.py:137\u001b[39m, in \u001b[36marun_with_retry\u001b[39m\u001b[34m(task, retry_policy, stream, match_cached_writes, configurable)\u001b[39m\n\u001b[32m    135\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m task.proc.ainvoke(task.input, config)\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    139\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\62687\\Miniconda3\\envs\\deep_research\\Lib\\site-packages\\langgraph\\utils\\runnable.py:672\u001b[39m, in \u001b[36mRunnableSeq.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    670\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    671\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m672\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(\n\u001b[32m    673\u001b[39m             step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs), context=context\n\u001b[32m    674\u001b[39m         )\n\u001b[32m    675\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    676\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\62687\\Miniconda3\\envs\\deep_research\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2919\u001b[39m, in \u001b[36mPregel.ainvoke\u001b[39m\u001b[34m(self, input, config, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, **kwargs)\u001b[39m\n\u001b[32m   2916\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   2917\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m2919\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.astream(\n\u001b[32m   2920\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   2921\u001b[39m     config,\n\u001b[32m   2922\u001b[39m     stream_mode=[\u001b[33m\"\u001b[39m\u001b[33mupdates\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   2923\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2924\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m stream_mode,\n\u001b[32m   2925\u001b[39m     print_mode=print_mode,\n\u001b[32m   2926\u001b[39m     output_keys=output_keys,\n\u001b[32m   2927\u001b[39m     interrupt_before=interrupt_before,\n\u001b[32m   2928\u001b[39m     interrupt_after=interrupt_after,\n\u001b[32m   2929\u001b[39m     **kwargs,\n\u001b[32m   2930\u001b[39m ):\n\u001b[32m   2931\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2932\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunk) == \u001b[32m2\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\62687\\Miniconda3\\envs\\deep_research\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2767\u001b[39m, in \u001b[36mPregel.astream\u001b[39m\u001b[34m(self, input, config, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2765\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m loop.amatch_cached_writes():\n\u001b[32m   2766\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2767\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.atick(\n\u001b[32m   2768\u001b[39m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop.tasks.values() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.writes],\n\u001b[32m   2769\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   2770\u001b[39m     get_waiter=get_waiter,\n\u001b[32m   2771\u001b[39m     schedule_task=loop.aaccept_push,\n\u001b[32m   2772\u001b[39m ):\n\u001b[32m   2773\u001b[39m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   2774\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m _output(\n\u001b[32m   2775\u001b[39m         stream_mode,\n\u001b[32m   2776\u001b[39m         print_mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2779\u001b[39m         asyncio.QueueEmpty,\n\u001b[32m   2780\u001b[39m     ):\n\u001b[32m   2781\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\62687\\Miniconda3\\envs\\deep_research\\Lib\\site-packages\\langgraph\\pregel\\runner.py:295\u001b[39m, in \u001b[36mPregelRunner.atick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    293\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(\n\u001b[32m    296\u001b[39m         t,\n\u001b[32m    297\u001b[39m         retry_policy,\n\u001b[32m    298\u001b[39m         stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    299\u001b[39m         configurable={\n\u001b[32m    300\u001b[39m             CONFIG_KEY_CALL: partial(\n\u001b[32m    301\u001b[39m                 _acall,\n\u001b[32m    302\u001b[39m                 weakref.ref(t),\n\u001b[32m    303\u001b[39m                 stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    304\u001b[39m                 retry_policy=retry_policy,\n\u001b[32m    305\u001b[39m                 futures=weakref.ref(futures),\n\u001b[32m    306\u001b[39m                 schedule_task=schedule_task,\n\u001b[32m    307\u001b[39m                 submit=\u001b[38;5;28mself\u001b[39m.submit,\n\u001b[32m    308\u001b[39m                 loop=loop,\n\u001b[32m    309\u001b[39m             ),\n\u001b[32m    310\u001b[39m         },\n\u001b[32m    311\u001b[39m     )\n\u001b[32m    312\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\62687\\Miniconda3\\envs\\deep_research\\Lib\\site-packages\\langgraph\\pregel\\retry.py:137\u001b[39m, in \u001b[36marun_with_retry\u001b[39m\u001b[34m(task, retry_policy, stream, match_cached_writes, configurable)\u001b[39m\n\u001b[32m    135\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m task.proc.ainvoke(task.input, config)\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    139\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\62687\\Miniconda3\\envs\\deep_research\\Lib\\site-packages\\langgraph\\utils\\runnable.py:672\u001b[39m, in \u001b[36mRunnableSeq.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    670\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    671\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m672\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(\n\u001b[32m    673\u001b[39m             step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs), context=context\n\u001b[32m    674\u001b[39m         )\n\u001b[32m    675\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    676\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\62687\\Miniconda3\\envs\\deep_research\\Lib\\site-packages\\langgraph\\utils\\runnable.py:440\u001b[39m, in \u001b[36mRunnableCallable.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    438\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m run_manager.on_chain_end(ret)\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     ret = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.afunc(*args, **kwargs)\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    442\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m ret.ainvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\62687\\Desktop\\个人\\实习相关\\标准化研究院实习\\open_deep_research\\src\\open_deep_research\\graph.py:277\u001b[39m, in \u001b[36msearch_web\u001b[39m\u001b[34m(state, config)\u001b[39m\n\u001b[32m    274\u001b[39m query_list = [query.search_query \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m search_queries]\n\u001b[32m    276\u001b[39m \u001b[38;5;66;03m# Search the web with parameters\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m source_str = \u001b[38;5;28;01mawait\u001b[39;00m select_and_execute_search(search_api, query_list, params_to_pass)\n\u001b[32m    278\u001b[39m source_str = reduce_source_str(source_str, max_tokens=\u001b[32m30000\u001b[39m)  \u001b[38;5;66;03m# Reduce source string if too long\u001b[39;00m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33msource_str\u001b[39m\u001b[33m\"\u001b[39m: source_str, \u001b[33m\"\u001b[39m\u001b[33msearch_iterations\u001b[39m\u001b[33m\"\u001b[39m: state[\u001b[33m\"\u001b[39m\u001b[33msearch_iterations\u001b[39m\u001b[33m\"\u001b[39m] + \u001b[32m1\u001b[39m}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\62687\\Desktop\\个人\\实习相关\\标准化研究院实习\\open_deep_research\\src\\open_deep_research\\utils.py:1546\u001b[39m, in \u001b[36mselect_and_execute_search\u001b[39m\u001b[34m(search_api, query_list, params_to_pass)\u001b[39m\n\u001b[32m   1530\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Select and execute the appropriate search API.\u001b[39;00m\n\u001b[32m   1531\u001b[39m \u001b[33;03m\u001b[39;00m\n\u001b[32m   1532\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1541\u001b[39m \u001b[33;03m    ValueError: If an unsupported search API is specified\u001b[39;00m\n\u001b[32m   1542\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1543\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m search_api == \u001b[33m\"\u001b[39m\u001b[33mtavily\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1544\u001b[39m     \u001b[38;5;66;03m# Tavily search tool used with both workflow and agent \u001b[39;00m\n\u001b[32m   1545\u001b[39m     \u001b[38;5;66;03m# and returns a formatted source string\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1546\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m tavily_search.ainvoke({\u001b[33m'\u001b[39m\u001b[33mqueries\u001b[39m\u001b[33m'\u001b[39m: query_list, **params_to_pass})\n\u001b[32m   1547\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m search_api == \u001b[33m\"\u001b[39m\u001b[33mduckduckgo\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1548\u001b[39m     \u001b[38;5;66;03m# DuckDuckGo search tool used with both workflow and agent \u001b[39;00m\n\u001b[32m   1549\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m duckduckgo_search.ainvoke({\u001b[33m'\u001b[39m\u001b[33msearch_queries\u001b[39m\u001b[33m'\u001b[39m: query_list})\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\62687\\Miniconda3\\envs\\deep_research\\Lib\\site-packages\\langchain_core\\tools\\structured.py:66\u001b[39m, in \u001b[36mStructuredTool.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.coroutine:\n\u001b[32m     63\u001b[39m     \u001b[38;5;66;03m# If the tool does not implement async, fall back to default implementation\u001b[39;00m\n\u001b[32m     64\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m run_in_executor(config, \u001b[38;5;28mself\u001b[39m.invoke, \u001b[38;5;28minput\u001b[39m, config, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\62687\\Miniconda3\\envs\\deep_research\\Lib\\site-packages\\langchain_core\\tools\\base.py:609\u001b[39m, in \u001b[36mBaseTool.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    601\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    602\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m    603\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    606\u001b[39m     **kwargs: Any,\n\u001b[32m    607\u001b[39m ) -> Any:\n\u001b[32m    608\u001b[39m     tool_input, kwargs = _prep_run_args(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m609\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.arun(tool_input, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\62687\\Miniconda3\\envs\\deep_research\\Lib\\site-packages\\langchain_core\\tools\\base.py:996\u001b[39m, in \u001b[36mBaseTool.arun\u001b[39m\u001b[34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[39m\n\u001b[32m    994\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error_to_raise:\n\u001b[32m    995\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m run_manager.on_tool_error(error_to_raise)\n\u001b[32m--> \u001b[39m\u001b[32m996\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error_to_raise\n\u001b[32m    998\u001b[39m output = _format_output(content, artifact, tool_call_id, \u001b[38;5;28mself\u001b[39m.name, status)\n\u001b[32m    999\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m run_manager.on_tool_end(output, color=color, name=\u001b[38;5;28mself\u001b[39m.name, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\62687\\Miniconda3\\envs\\deep_research\\Lib\\site-packages\\langchain_core\\tools\\base.py:965\u001b[39m, in \u001b[36mBaseTool.arun\u001b[39m\u001b[34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[39m\n\u001b[32m    962\u001b[39m         tool_kwargs[config_param] = config\n\u001b[32m    964\u001b[39m     coro = \u001b[38;5;28mself\u001b[39m._arun(*tool_args, **tool_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m coro_with_context(coro, context)\n\u001b[32m    966\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.response_format == \u001b[33m\"\u001b[39m\u001b[33mcontent_and_artifact\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    967\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(response) != \u001b[32m2\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\62687\\Miniconda3\\envs\\deep_research\\Lib\\site-packages\\langchain_core\\tools\\structured.py:110\u001b[39m, in \u001b[36mStructuredTool._arun\u001b[39m\u001b[34m(self, config, run_manager, *args, **kwargs)\u001b[39m\n\u001b[32m    108\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config_param := _get_runnable_config_param(\u001b[38;5;28mself\u001b[39m.coroutine):\n\u001b[32m    109\u001b[39m         kwargs[config_param] = config\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.coroutine(*args, **kwargs)\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# If self.coroutine is None, then this will delegate to the default\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# implementation which is expected to delegate to _run on a separate thread.\u001b[39;00m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()._arun(\n\u001b[32m    115\u001b[39m     *args, config=config, run_manager=run_manager, **kwargs\n\u001b[32m    116\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\62687\\Desktop\\个人\\实习相关\\标准化研究院实习\\open_deep_research\\src\\open_deep_research\\utils.py:1404\u001b[39m, in \u001b[36mtavily_search\u001b[39m\u001b[34m(queries, max_results, topic, max_tokens, config)\u001b[39m\n\u001b[32m   1392\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1393\u001b[39m \u001b[33;03mFetches results from Tavily search API.\u001b[39;00m\n\u001b[32m   1394\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1401\u001b[39m \u001b[33;03m    str: A formatted string of search results\u001b[39;00m\n\u001b[32m   1402\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1403\u001b[39m \u001b[38;5;66;03m# Use tavily_search_async with include_raw_content=True to get content directly\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1404\u001b[39m search_results = \u001b[38;5;28;01mawait\u001b[39;00m tavily_search_async(\n\u001b[32m   1405\u001b[39m     queries,\n\u001b[32m   1406\u001b[39m     max_results=max_results,\n\u001b[32m   1407\u001b[39m     topic=topic,\n\u001b[32m   1408\u001b[39m     include_raw_content=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1409\u001b[39m )\n\u001b[32m   1411\u001b[39m \u001b[38;5;66;03m# Format the search results directly using the raw_content already provided\u001b[39;00m\n\u001b[32m   1412\u001b[39m formatted_output = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m搜索结果: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\62687\\Miniconda3\\envs\\deep_research\\Lib\\site-packages\\langsmith\\run_helpers.py:563\u001b[39m, in \u001b[36mtraceable.<locals>.decorator.<locals>.async_wrapper\u001b[39m\u001b[34m(langsmith_extra, *args, **kwargs)\u001b[39m\n\u001b[32m    561\u001b[39m fr_coro = func(*args, **kwargs)\n\u001b[32m    562\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m accepts_context:\n\u001b[32m--> \u001b[39m\u001b[32m563\u001b[39m     function_result = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m    564\u001b[39m         fr_coro, context=run_container[\u001b[33m\"\u001b[39m\u001b[33mcontext\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    565\u001b[39m     )\n\u001b[32m    566\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    567\u001b[39m     \u001b[38;5;66;03m# Python < 3.11\u001b[39;00m\n\u001b[32m    568\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m tracing_context(\n\u001b[32m    569\u001b[39m         **get_tracing_context(run_container[\u001b[33m\"\u001b[39m\u001b[33mcontext\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    570\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:31\u001b[39m, in \u001b[36mtavily_search_async\u001b[39m\u001b[34m(search_queries, max_results, topic, include_raw_content)\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'AsyncTavilyClient' is not defined",
      "During task with name 'search_web' and id 'bdcab1dc-40b3-105a-8c5e-2ad36b88d915'",
      "During task with name 'build_section_with_web_research' and id '5db59590-bf47-95ae-087d-24b1144e1697'"
     ]
    }
   ],
   "source": [
    "# Approve the final plan and execute the report generation\n",
    "# This triggers the research and writing phases for all sections\n",
    "\n",
    "# The system will now:\n",
    "# 1. Research each section topic\n",
    "# 2. Generate content with citations\n",
    "# 3. Create introduction and conclusion\n",
    "# 4. Compile the final report\n",
    "\n",
    "async for event in graph.astream(Command(resume=True), thread, stream_mode=\"updates\"):\n",
    "    print(event)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Introduction  \n",
       "Large language models excel at reasoning, but without structured access to the outside world they remain isolated. The Model Context Protocol (MCP) bridges this gap, defining an open, vendor‑neutral way for models to tap files, databases, APIs, and other tools through simple JSON‑RPC exchanges. This report walks developers through the protocol’s architecture, surveys real‑world MCP servers that showcase its flexibility, and contrasts MCP with Google’s emerging Agent‑to‑Agent (A2A) standard. By the end, you should know when, why, and how to weave MCP into your own agentic systems.\n",
       "\n",
       "## MCP Architectural Overview for Developers\n",
       "\n",
       "MCP uses a client‑host‑server model: a host process spawns isolated clients, and every client keeps a 1‑to‑1, stateful session with a single server that exposes prompts, resources, and tools through JSON‑RPC 2.0 messages [1][5].  \n",
       "\n",
       "A session passes through three phases — initialize, operation, shutdown. The client begins with an initialize request that lists its protocolVersion and capabilities; the server replies with a compatible version and its own capabilities. After the client’s initialized notification, both sides may exchange requests, responses, or one‑way notifications under the agreed capabilities [2].  \n",
       "\n",
       "Two official transports exist. Stdio is ideal for local child processes, while HTTP (SSE/“streamable HTTP”) supports multi‑client, remote scenarios. Both must preserve JSON‑RPC framing, and servers should validate Origin headers, bind to localhost where possible, and apply TLS or authentication to block DNS‑rebind or similar attacks [1][3].  \n",
       "\n",
       "To integrate MCP, developers can:  \n",
       "1) implement a server that registers needed primitives and advertises them in initialize.result.capabilities;  \n",
       "2) validate all inputs and set reasonable timeouts;  \n",
       "3) or consume existing servers via SDKs—select a transport, send initialize, then invoke or subscribe to tools/resources exactly as negotiated [4][5].  \n",
       "\n",
       "### Sources  \n",
       "[1] MCP Protocol Specification: https://www.claudemcp.com/specification  \n",
       "[2] Lifecycle – Model Context Protocol: https://modelcontextprotocol.info/specification/draft/basic/lifecycle/  \n",
       "[3] Transports – Model Context Protocol: https://modelcontextprotocol.io/specification/2025-03-26/basic/transports  \n",
       "[4] Core Architecture – Model Context Protocol: https://modelcontextprotocol.io/docs/concepts/architecture  \n",
       "[5] Architecture – Model Context Protocol Specification: https://spec.modelcontextprotocol.io/specification/2025-03-26/architecture/\n",
       "\n",
       "## Ecosystem Spotlight: Notable MCP Servers\n",
       "\n",
       "Hundreds of MCP servers now exist, spanning core data access, commercial platforms, and hobby projects—proof that the protocol can wrap almost any tool or API [1][2].\n",
       "\n",
       "Reference servers maintained by Anthropic demonstrate the basics.  Filesystem, PostgreSQL, Git, and Slack servers cover file I/O, SQL queries, repository ops, and chat workflows.  Developers can launch them in seconds with commands like  \n",
       "`npx -y @modelcontextprotocol/server-filesystem` (TypeScript) or `uvx mcp-server-git` (Python) and then point any MCP‑aware client, such as Claude Desktop, at the spawned process [1].\n",
       "\n",
       "Platform vendors are adding “first‑party” connectors.  Microsoft cites the GitHub MCP Server and a Playwright browser‑automation server as popular examples that let C# or .NET apps drive code reviews or end‑to‑end tests through a uniform interface [3].  Other partner servers—e.g., Cloudflare for edge resources or Stripe for payments—expose full product APIs while still enforcing user approval through MCP’s tool‑calling flow [2].\n",
       "\n",
       "Community builders rapidly fill remaining gaps.  Docker and Kubernetes servers give agents controlled shell access; Snowflake, Neon, and Qdrant handle cloud databases; Todoist and Obsidian servers tackle personal productivity.  Because every server follows the same JSON‑RPC schema and ships as a small CLI, developers can fork an existing TypeScript or Python implementation and swap in their own SDK calls to create new connectors in hours, not weeks [2].  \n",
       "\n",
       "### Sources  \n",
       "[1] Example Servers – Model Context Protocol: https://modelcontextprotocol.io/examples  \n",
       "[2] Model Context Protocol Servers Repository: https://github.com/madhukarkumar/anthropic-mcp-servers  \n",
       "[3] Microsoft partners with Anthropic to create official C# SDK for Model Context Protocol: https://devblogs.microsoft.com/blog/microsoft-partners-with-anthropic-to-create-official-c-sdk-for-model-context-protocol\n",
       "\n",
       "## Agent‑to‑Agent (A2A) Protocol and Comparison with MCP  \n",
       "\n",
       "Google’s Agent‑to‑Agent (A2A) protocol, announced in April 2025, gives autonomous agents a common way to talk directly across vendors and clouds [2]. Its goal is to let one “client” agent delegate work to a “remote” agent without sharing internal code or memory, enabling true multi‑agent systems.  \n",
       "\n",
       "Discovery starts with a JSON Agent Card served at /.well‑known/agent.json, which lists version, skills and endpoints [3]. After discovery, the client opens a Task—an atomic unit that moves through states and exchanges Messages and multimodal Artifacts. HTTP request/response, Server‑Sent Events, or push notifications are chosen based on task length to stream progress safely [2].  \n",
       "\n",
       "Anthropic’s Model Context Protocol (MCP) tackles a different layer: it links a single language model to external tools and data through a Host‑Client‑Server triad, exposing Resources, Tools and Prompts over JSON‑RPC [1]. Communication is model‑to‑tool, not agent‑to‑agent.  \n",
       "\n",
       "Google therefore calls A2A “complementary” to MCP: use MCP to give each agent the data and actions it needs; use A2A to let those empowered agents discover one another, coordinate plans and exchange results [1]. In practice, developers might pipe an A2A task that, mid‑flow, invokes an MCP tool or serve an MCP connector as an A2A remote agent, showing the standards can interlock instead of compete.  \n",
       "\n",
       "### Sources  \n",
       "[1] MCP vs A2A: Comprehensive Comparison of AI Agent Protocols: https://www.toolworthy.ai/blog/mcp-vs-a2a-protocol-comparison  \n",
       "[2] Google A2A vs MCP: The New Protocol Standard Developers Need to Know: https://www.trickle.so/blog/google-a2a-vs-mcp  \n",
       "[3] A2A vs MCP: Comparing AI Standards for Agent Interoperability: https://www.ikangai.com/a2a-vs-mcp-ai-standards/\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "Model Context Protocol (MCP) secures a model’s immediate tool belt, while Google’s Agent‑to‑Agent (A2A) protocol enables those empowered agents to find and hire one another. Their scopes differ but interlock, giving developers a layered recipe for robust, multi‑agent applications.\n",
       "\n",
       "| Aspect | MCP | A2A |\n",
       "| --- | --- | --- |\n",
       "| Layer | Model‑to‑tool RPC | Agent‑to‑agent orchestration |\n",
       "| Session start | `initialize` handshake | Task creation lifecycle |\n",
       "| Discovery | Client‑supplied server URI | `/.well‑known/agent.json` card |\n",
       "| Streaming | Stdio or HTTP/SSE | HTTP, SSE, or push |\n",
       "| Best fit | Embed filesystems, DBs, SaaS APIs into one agent | Delegate subtasks across clouds or vendors |\n",
       "\n",
       "Next steps: prototype an A2A task that internally calls an MCP PostgreSQL server; harden both layers with TLS and capability scoping; finally, contribute a new open‑source MCP connector to accelerate community adoption."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the final generated report\n",
    "# Retrieve the completed report from the graph's state and format it for display\n",
    "\n",
    "final_state = graph.get_state(thread)\n",
    "report = final_state.values.get('final_report')\n",
    "Markdown(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trace: \n",
    "\n",
    "> Note: uses 80k tokens \n",
    "\n",
    "https://smith.langchain.com/public/31eca7c9-beae-42a3-bef4-5bce9488d7be/r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
