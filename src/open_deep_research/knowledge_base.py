import os
import json
import time
import asyncio
import logging
import datetime
import numpy as np
import threading
from sklearn.metrics.pairwise import cosine_similarity


import hashlib
import pickle
from pathlib import Path
from typing import List, Optional, Dict, Any
from collections import defaultdict


from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.vectorstores import VectorStore


class KnowledgeBaseManager:
    def __init__(self, embeddings: Embeddings):
        self.embeddings = embeddings
        self.similarity_threshold = 0.91  # ç›¸ä¼¼åº¦é˜ˆå€¼
        self.project_root = Path(__file__).parent.parent.parent
        self.kb_dir = self.project_root / "knowledge_base"
        # self.kb_dir.mkdir(exist_ok=True, parents=True)
        self.index_file = self.kb_dir / "kb_index.json"

        # ä¸ºæ–‡ä»¶æ“ä½œæ·»åŠ å¼‚æ­¥é”
        self._index_lock = asyncio.Lock()
        self._index_thread_lock = threading.Lock()  # æ·»åŠ çº¿ç¨‹é”
        self._kb_locks = {}  # ä¸ºæ¯ä¸ªçŸ¥è¯†åº“æ–‡ä»¶å•ç‹¬åŠ é”
        self._locks_lock = asyncio.Lock()  # ä¿æŠ¤é”å­—å…¸çš„é”

    async def _get_kb_lock(self, kb_id: str) -> asyncio.Lock:
        """è·å–ç‰¹å®šçŸ¥è¯†åº“çš„é”"""
        async with self._locks_lock:
            if kb_id not in self._kb_locks:
                self._kb_locks[kb_id] = asyncio.Lock()
            return self._kb_locks[kb_id]

    async def _load_kb_index(self) -> Dict[str, Any]:
        """åŠ è½½çŸ¥è¯†åº“ç´¢å¼•"""
        async with self._index_lock:
            if not self.index_file.exists():
                return {"knowledge_bases": []}

            def read_json():
                # ä½¿ç”¨çº¿ç¨‹é”ä¿æŠ¤åŒæ­¥æ–‡ä»¶æ“ä½œ
                with self._index_thread_lock:
                    max_retries = 3
                    for attempt in range(max_retries):
                        try:
                            # ç¡®ä¿ç›®å½•å­˜åœ¨
                            self.kb_dir.mkdir(exist_ok=True, parents=True)

                            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                            if not self.index_file.exists():
                                return {"knowledge_bases": []}

                            # æ£€æŸ¥æ–‡ä»¶æƒé™
                            if not os.access(self.index_file, os.R_OK):
                                logging.warning(f"No read permission for {self.index_file}")
                                return {"knowledge_bases": []}

                            with open(self.index_file, "r", encoding="utf-8") as f:
                                content = f.read()
                                if not content.strip():
                                    return {"knowledge_bases": []}
                                return json.loads(content)

                        except PermissionError as e:
                            if attempt < max_retries - 1:
                                logging.warning(f"Permission denied on attempt {attempt + 1}, retrying...")
                                time.sleep(0.1 * (attempt + 1))  # é€’å¢ç­‰å¾…æ—¶é—´
                                continue
                            logging.error(f"Permission denied after {max_retries} attempts: {e}")
                            return {"knowledge_bases": []}
                        except json.JSONDecodeError as e:
                            logging.warning(f"JSON decode error: {e}, returning empty index")
                            return {"knowledge_bases": []}
                        except Exception as e:
                            logging.error(f"Unexpected error reading index: {e}")
                            if attempt < max_retries - 1:
                                time.sleep(0.1 * (attempt + 1))
                                continue
                            return {"knowledge_bases": []}

                    return {"knowledge_bases": []}

            return await asyncio.to_thread(read_json)

    async def _save_kb_index(self, index_data: Dict[str, Any]) -> None:
        """ä¿å­˜çŸ¥è¯†åº“ç´¢å¼•"""
        async with self._index_lock:

            def write_json():
                # ä½¿ç”¨çº¿ç¨‹é”ä¿æŠ¤åŒæ­¥æ–‡ä»¶æ“ä½œ
                with self._index_thread_lock:
                    # ç¡®ä¿ç›®å½•å­˜åœ¨
                    self.kb_dir.mkdir(exist_ok=True, parents=True)

                    # ç”Ÿæˆå”¯ä¸€çš„ä¸´æ—¶æ–‡ä»¶å
                    timestamp = int(time.time() * 1000000)
                    process_id = os.getpid()
                    thread_id = threading.get_ident()
                    temp_file = self.index_file.with_suffix(f".tmp.{process_id}.{thread_id}.{timestamp}")

                    max_retries = 3
                    for attempt in range(max_retries):
                        try:
                            # å†™å…¥ä¸´æ—¶æ–‡ä»¶
                            with open(temp_file, "w", encoding="utf-8") as f:
                                json.dump(index_data, f, ensure_ascii=False, indent=2)

                            # åŸå­æ€§æ›¿æ¢
                            temp_file.replace(self.index_file)
                            return

                        except PermissionError as e:
                            if attempt < max_retries - 1:
                                logging.warning(f"Permission denied writing on attempt {attempt + 1}, retrying...")
                                if temp_file.exists():
                                    temp_file.unlink()
                                time.sleep(0.1 * (attempt + 1))
                                continue
                            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                            if temp_file.exists():
                                temp_file.unlink()
                            raise e
                        except Exception as e:
                            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                            if temp_file.exists():
                                temp_file.unlink()
                            if attempt < max_retries - 1:
                                time.sleep(0.1 * (attempt + 1))
                                continue
                            raise e

            await asyncio.to_thread(write_json)

    def _compute_query_embedding(self, query: str) -> List[float]:
        """è®¡ç®—æŸ¥è¯¢çš„å‘é‡è¡¨ç¤º"""
        return self.embeddings.embed_query(query)

    async def find_best_knowledge_base(self, query: str) -> Optional[tuple[str, float]]:
        """æ‰¾åˆ°æœ€åŒ¹é…çš„çŸ¥è¯†åº“"""
        index_data = await self._load_kb_index()

        if not index_data["knowledge_bases"]:
            return None

        query_embedding = np.array([self._compute_query_embedding(query)])

        best_kb_id = ""
        best_similarity = 0.0

        for kb_info in index_data["knowledge_bases"]:
            # è®¡ç®—ä¸çŸ¥è¯†åº“ä»£è¡¨æ€§æŸ¥è¯¢çš„ç›¸ä¼¼åº¦
            kb_embedding = np.array([kb_info["query_embedding"]])
            similarity: float = cosine_similarity(query_embedding, kb_embedding)[0][0]

            if similarity > best_similarity:
                best_similarity = similarity
                best_kb_id: str = kb_info["kb_id"]

        if best_similarity >= self.similarity_threshold:
            return best_kb_id, best_similarity

        return None

    async def create_knowledge_base(self, query: str) -> str:
        """åˆ›å»ºæ–°çš„çŸ¥è¯†åº“"""
        kb_id = hashlib.md5(f"{query}_{datetime.datetime.now().isoformat()}".encode()).hexdigest()[:12]
        query_embedding = self._compute_query_embedding(query)

        index_data = await self._load_kb_index()
        index_data["knowledge_bases"].append(
            {
                "kb_id": kb_id,
                "representative_query": query,
                "query_embedding": query_embedding,
                "created_at": datetime.datetime.now().isoformat(),
                "query_count": 1,
                "doc_count": 0,
                "related_queries": [query],
            }
        )

        await self._save_kb_index(index_data)
        return kb_id

    async def update_knowledge_base_info(self, kb_id: str, query: str, doc_count: int):
        """æ›´æ–°çŸ¥è¯†åº“ä¿¡æ¯"""
        index_data = await self._load_kb_index()

        for kb_info in index_data["knowledge_bases"]:
            if kb_info["kb_id"] == kb_id:
                kb_info["query_count"] += 1
                kb_info["doc_count"] = doc_count
                kb_info["related_queries"].append(query)
                kb_info["last_updated"] = datetime.datetime.now().isoformat()

                # å¦‚æœæŸ¥è¯¢æ•°é‡è¶…è¿‡é˜ˆå€¼ï¼Œé‡æ–°è®¡ç®—ä»£è¡¨æ€§å‘é‡
                if kb_info["query_count"] % 5 == 0:
                    all_queries = kb_info["related_queries"]
                    all_embeddings = [self._compute_query_embedding(q) for q in all_queries]
                    # ä½¿ç”¨å¹³å‡å‘é‡ä½œä¸ºä»£è¡¨æ€§å‘é‡
                    kb_info["query_embedding"] = np.mean(all_embeddings, axis=0).tolist()

                break

        await self._save_kb_index(index_data)

    def get_knowledge_base_path(self, kb_id: str) -> Path:
        """è·å–çŸ¥è¯†åº“æ–‡ä»¶è·¯å¾„"""
        return self.kb_dir / f"kb_{kb_id}.pkl"

    async def save_knowledge_base(self, kb_id: str, vector_store: VectorStore, query: str, documents: List[Document]):
        """ä¿å­˜çŸ¥è¯†åº“"""
        kb_lock = await self._get_kb_lock(kb_id)
        async with kb_lock:

            def write_pickle():
                # ç¡®ä¿ç›®å½•å­˜åœ¨
                self.kb_dir.mkdir(exist_ok=True, parents=True)

                timestamp = int(time.time() * 1000000)
                process_id = os.getpid()
                thread_id = threading.get_ident()
                kb_path = self.get_knowledge_base_path(kb_id)
                temp_path = kb_path.with_suffix(f".tmp.{process_id}.{thread_id}.{timestamp}")

                save_data = {
                    "vector_store": vector_store,
                    "documents": documents,
                    "kb_id": kb_id,
                    "last_query": query,
                    "updated_at": datetime.datetime.now().isoformat(),
                    "doc_count": len(documents),
                }

                max_retries = 3
                for attempt in range(max_retries):
                    try:
                        # å†™å…¥ä¸´æ—¶æ–‡ä»¶
                        with open(temp_path, "wb") as f:
                            pickle.dump(save_data, f)

                        # Windows ä¸‹çš„å®‰å…¨æ›¿æ¢æ“ä½œ
                        if kb_path.exists():
                            # å¦‚æœç›®æ ‡æ–‡ä»¶å­˜åœ¨ï¼Œå…ˆå¤‡ä»½å†æ›¿æ¢
                            backup_path = kb_path.with_suffix(f".bak.{timestamp}")
                            try:
                                kb_path.rename(backup_path)
                                temp_path.rename(kb_path)
                                # åˆ é™¤å¤‡ä»½æ–‡ä»¶
                                backup_path.unlink()
                            except Exception as e:
                                # å¦‚æœæ›¿æ¢å¤±è´¥ï¼Œæ¢å¤å¤‡ä»½
                                if backup_path.exists():
                                    backup_path.rename(kb_path)
                                raise e
                        else:
                            # ç›®æ ‡æ–‡ä»¶ä¸å­˜åœ¨ï¼Œç›´æ¥é‡å‘½å
                            temp_path.rename(kb_path)

                        logging.info(f"Successfully saved knowledge base {kb_id}")
                        return

                    except PermissionError as e:
                        if attempt < max_retries - 1:
                            logging.warning(f"Permission denied on attempt {attempt + 1}, retrying...")
                            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                            if temp_path.exists():
                                temp_path.unlink()
                            time.sleep(0.2 * (attempt + 1))  # é€’å¢ç­‰å¾…æ—¶é—´
                            continue
                        logging.error(f"Permission denied after {max_retries} attempts: {e}")
                        raise e
                    except Exception as e:
                        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                        if temp_path.exists():
                            temp_path.unlink()
                        if attempt < max_retries - 1:
                            time.sleep(0.1 * (attempt + 1))
                            continue
                        logging.error(f"Failed to save knowledge base: {e}")
                        raise e

            try:
                await asyncio.to_thread(write_pickle)
                await self.update_knowledge_base_info(kb_id, query, len(documents))

            except Exception as e:
                logging.error(f"Failed to save knowledge base {kb_id}: {e}")
                raise e

    async def load_or_create_knowledge_base(
        self, query: str
    ) -> tuple[Optional[VectorStore], List[Document], str, bool]:
        """
        åŠ è½½æˆ–åˆ›å»ºçŸ¥è¯†åº“

        Returns:
            Tuple[vector_store, documents, kb_id, is_new]
        """
        # å°è¯•æ‰¾åˆ°æœ€åŒ¹é…çš„çŸ¥è¯†åº“
        match_result = await self.find_best_knowledge_base(query)

        if match_result:
            kb_id, similarity = match_result
            logging.info(f"Found matching knowledge base {kb_id} with similarity {similarity:.3f}")

            kb_lock = await self._get_kb_lock(kb_id)
            async with kb_lock:
                # åŠ è½½ç°æœ‰çŸ¥è¯†åº“
                kb_path = self.get_knowledge_base_path(kb_id)
                if kb_path.exists():
                    try:
                        save_data = await asyncio.to_thread(lambda: pickle.load(open(kb_path, "rb")))

                        vector_store = save_data["vector_store"]
                        documents = save_data["documents"]

                        logging.info(f"Loaded existing knowledge base {kb_id} with {len(documents)} documents")
                        return (vector_store, documents, kb_id, False)  # is_new = Falseï¼Œè¡¨ç¤ºä½¿ç”¨ç°æœ‰çŸ¥è¯†åº“
                    except Exception as e:
                        logging.error(f"Failed to load knowledge base {kb_id}: {e}")

        # åˆ›å»ºæ–°çŸ¥è¯†åº“
        kb_id = await self.create_knowledge_base(query)
        logging.info(f"No Matching results, created new knowledge base {kb_id} for query: {query}")
        return None, [], kb_id, True  # is_new = Trueï¼Œè¡¨ç¤ºæ–°å»ºçŸ¥è¯†åº“

    async def print_all_knowledge_bases(
        self, show_full_content: bool = False, max_content_length: int = 200, max_url_num: int = 3
    ) -> None:
        """
        æ‰“å°æ‰€æœ‰çŸ¥è¯†åº“çš„è¯¦ç»†ä¿¡æ¯

        Args:
            show_full_content (bool): æ˜¯å¦æ˜¾ç¤ºå®Œæ•´çš„æ–‡æ¡£å†…å®¹
            max_content_length (int): å½“show_full_content=Falseæ—¶ï¼Œæ¯ä¸ªæ–‡æ¡£æ˜¾ç¤ºçš„æœ€å¤§å­—ç¬¦æ•°
        """
        index_data = await self._load_kb_index()

        if not index_data["knowledge_bases"]:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•çŸ¥è¯†åº“")
            return

        print("=" * 100)
        print(f"ğŸ“š çŸ¥è¯†åº“æ€»è§ˆ ({len(index_data['knowledge_bases'])} ä¸ªçŸ¥è¯†åº“)")
        print("=" * 100)

        for i, kb_info in enumerate(index_data["knowledge_bases"], 1):
            kb_id = kb_info["kb_id"]

            print(f"\nğŸ›ï¸  çŸ¥è¯†åº“ {i}: {kb_id}")
            print("-" * 80)
            print(f"ğŸ“ ä»£è¡¨æ€§æŸ¥è¯¢: {kb_info['representative_query']}")
            print(f"ğŸ“… åˆ›å»ºæ—¶é—´: {kb_info['created_at']}")
            print(f"ğŸ”„ æŸ¥è¯¢æ¬¡æ•°: {kb_info['query_count']}")
            print(f"ğŸ“„ æ–‡æ¡£æ•°é‡: {kb_info['doc_count']}")
            print(f"ğŸ•’ æœ€åæ›´æ–°: {kb_info.get('last_updated', 'æœªæ›´æ–°')}")

            # æ˜¾ç¤ºç›¸å…³æŸ¥è¯¢
            if len(kb_info["related_queries"]) > 1:
                print(f"ğŸ”— ç›¸å…³æŸ¥è¯¢ ({len(kb_info['related_queries'])}):")
                for j, query in enumerate(kb_info["related_queries"][:5], 1):  # æœ€å¤šæ˜¾ç¤º5ä¸ª
                    print(f"  {j}. {query}")
                if len(kb_info["related_queries"]) > 5:
                    print(f"  ... è¿˜æœ‰ {len(kb_info['related_queries']) - 5} ä¸ªæŸ¥è¯¢")

            # å°è¯•åŠ è½½å¹¶æ˜¾ç¤ºæ–‡æ¡£å†…å®¹
            kb_path = self.get_knowledge_base_path(kb_id)
            if kb_path.exists():
                try:
                    with open(kb_path, "rb") as f:
                        save_data = pickle.load(f)

                    documents = save_data.get("documents", [])
                    if documents:
                        print(f"\nğŸ“š æ–‡æ¡£å†…å®¹ ({len(documents)} ä¸ªæ–‡æ¡£):")

                        # æŒ‰URLåˆ†ç»„æ˜¾ç¤ºæ–‡æ¡£
                        url_groups = defaultdict(list)
                        for doc in documents:
                            url = doc.metadata.get("url", "æœªçŸ¥URL")
                            url_groups[url].append(doc)

                        for i, (url, docs) in enumerate(list(url_groups.items())[:max_url_num], 1):
                            print(f"\nğŸ“š æ–‡æ¡£ {i}\n" + "-" * 80)
                            print(f"\nğŸŒ æ¥æº: {url}")
                            print(f"ğŸ“‘ æ ‡é¢˜: {docs[0].metadata.get('title', 'æœªçŸ¥æ ‡é¢˜')}")
                            print(f"ğŸ“Š æ–‡æ¡£ç‰‡æ®µæ•°: {len(docs)}")

                            # æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ–‡æ¡£ç‰‡æ®µçš„å†…å®¹
                            if docs:
                                content = docs[0].page_content
                                if show_full_content:
                                    print(f"ğŸ“„ å†…å®¹:\n```\n{content}\n```")
                                else:
                                    if len(content) > max_content_length:
                                        truncated_content = content[:max_content_length] + "..."
                                        print(f"ğŸ“„ å†…å®¹é¢„è§ˆ:\n```\n{truncated_content}\n```")
                                    else:
                                        print(f"ğŸ“„ å†…å®¹:\n```\n{content}\n```")

                        if len(url_groups) > max_url_num:
                            print(f"\nğŸ’¡ è¿˜æœ‰ {len(url_groups) - max_url_num} ä¸ªå…¶ä»–æ¥æºçš„æ–‡æ¡£...")
                    else:
                        print("\nâŒ çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ–‡æ¡£å†…å®¹")

                except Exception as e:
                    print(f"\nâŒ æ— æ³•åŠ è½½çŸ¥è¯†åº“æ–‡ä»¶: {e}")
            else:
                print(f"\nâŒ çŸ¥è¯†åº“æ–‡ä»¶ä¸å­˜åœ¨: {kb_path}")

            print("\n" + "=" * 100)

    async def print_knowledge_base_summary(self) -> None:
        """æ‰“å°çŸ¥è¯†åº“çš„ç®€è¦ç»Ÿè®¡ä¿¡æ¯"""
        index_data = await self._load_kb_index()

        if not index_data["knowledge_bases"]:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•çŸ¥è¯†åº“")
            return

        total_kbs = len(index_data["knowledge_bases"])
        total_docs = sum(kb["doc_count"] for kb in index_data["knowledge_bases"])
        total_queries = sum(kb["query_count"] for kb in index_data["knowledge_bases"])

        print("ğŸ“Š çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯")
        print("=" * 50)
        print(f"ğŸ›ï¸  æ€»çŸ¥è¯†åº“æ•°é‡: {total_kbs}")
        print(f"ğŸ“„ æ€»æ–‡æ¡£æ•°é‡: {total_docs}")
        print(f"ğŸ” æ€»æŸ¥è¯¢æ¬¡æ•°: {total_queries}")
        print(f"ğŸ“ˆ å¹³å‡æ¯ä¸ªçŸ¥è¯†åº“æ–‡æ¡£æ•°: {total_docs / total_kbs:.1f}")
        print(f"ğŸ”„ å¹³å‡æ¯ä¸ªçŸ¥è¯†åº“æŸ¥è¯¢æ¬¡æ•°: {total_queries / total_kbs:.1f}")

        # æ˜¾ç¤ºæœ€æ´»è·ƒçš„çŸ¥è¯†åº“
        most_used_kb = max(index_data["knowledge_bases"], key=lambda x: x["query_count"])
        largest_kb = max(index_data["knowledge_bases"], key=lambda x: x["doc_count"])

        print(f"\nğŸ† æœ€æ´»è·ƒçŸ¥è¯†åº“:")
        print(f"   æŸ¥è¯¢: {most_used_kb['representative_query']}")
        print(f"   ä½¿ç”¨æ¬¡æ•°: {most_used_kb['query_count']}")

        print(f"\nğŸ“š æœ€å¤§çŸ¥è¯†åº“:")
        print(f"   æŸ¥è¯¢: {largest_kb['representative_query']}")
        print(f"   æ–‡æ¡£æ•°é‡: {largest_kb['doc_count']}")

    async def inspect_knowledge_base(
        self, kb_id: str, show_full_content: bool = True, max_content_length: int = 500
    ) -> None:
        """
        æŸ¥çœ‹ç‰¹å®šçŸ¥è¯†åº“çš„è¯¦ç»†ä¿¡æ¯

        Args:
            kb_id (str): çŸ¥è¯†åº“ID
            show_full_content (bool): æ˜¯å¦æ˜¾ç¤ºå®Œæ•´çš„æ–‡æ¡£å†…å®¹
            max_content_length (int): å½“show_full_content=Falseæ—¶ï¼Œæ¯ä¸ªæ–‡æ¡£æ˜¾ç¤ºçš„æœ€å¤§å­—ç¬¦æ•°
        """
        # å…ˆä»ç´¢å¼•ä¸­æŸ¥æ‰¾çŸ¥è¯†åº“ä¿¡æ¯
        index_data = await self._load_kb_index()
        kb_info = None

        for kb in index_data["knowledge_bases"]:
            if kb["kb_id"] == kb_id:
                kb_info = kb
                break

        if not kb_info:
            print(f"âŒ æœªæ‰¾åˆ°çŸ¥è¯†åº“ ID: {kb_id}")
            return

        kb_path = self.get_knowledge_base_path(kb_id)

        if not kb_path.exists():
            print(f"âŒ çŸ¥è¯†åº“æ–‡ä»¶ä¸å­˜åœ¨: {kb_path}")
            return

        try:
            with open(kb_path, "rb") as f:
                save_data = pickle.load(f)

            print("=" * 80)
            print(f"ğŸ“š çŸ¥è¯†åº“è¯¦ç»†ä¿¡æ¯: {kb_id}")
            print("=" * 80)
            print(f"ğŸ“ ä»£è¡¨æ€§æŸ¥è¯¢: {kb_info['representative_query']}")
            print(f"ğŸ“… åˆ›å»ºæ—¶é—´: {kb_info['created_at']}")
            print(f"ğŸ”„ æŸ¥è¯¢æ¬¡æ•°: {kb_info['query_count']}")
            print(f"ğŸ“„ æ–‡æ¡£æ•°é‡: {save_data.get('doc_count', len(save_data.get('documents', [])))}")
            print(f"ğŸ•’ æœ€åæ›´æ–°: {save_data.get('updated_at', 'æœªçŸ¥')}")
            print(f"ğŸ“ æ–‡ä»¶è·¯å¾„: {kb_path}")
            print(f"ğŸ’¾ æ–‡ä»¶å¤§å°: {kb_path.stat().st_size / 1024:.2f} KB")

            # æ˜¾ç¤ºæ‰€æœ‰ç›¸å…³æŸ¥è¯¢
            print(f"\nğŸ”— æ‰€æœ‰ç›¸å…³æŸ¥è¯¢ ({len(kb_info['related_queries'])}):")
            for i, query in enumerate(kb_info["related_queries"], 1):
                print(f"  {i:2d}. {query}")

            documents = save_data.get("documents", [])
            if not documents:
                print("\nâŒ çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ–‡æ¡£")
                return

            print(f"\nğŸ“„ æ–‡æ¡£è¯¦æƒ… ({len(documents)} ä¸ªæ–‡æ¡£):")
            print("=" * 80)

            for i, doc in enumerate(documents, 1):
                print(f"\nğŸ“‘ æ–‡æ¡£ {i}:")
                print("-" * 60)

                # æ˜¾ç¤ºå…ƒæ•°æ®
                if hasattr(doc, "metadata") and doc.metadata:
                    print("ğŸ“‹ å…ƒæ•°æ®:")
                    for key, value in doc.metadata.items():
                        print(f"  {key}: {value}")

                # æ˜¾ç¤ºæ–‡æ¡£å†…å®¹
                if hasattr(doc, "page_content"):
                    content = doc.page_content
                    print(f"\nğŸ“ å†…å®¹ (é•¿åº¦: {len(content)} å­—ç¬¦):")

                    if show_full_content:
                        print(f"```\n{content}\n```")
                    else:
                        # æ˜¾ç¤ºæˆªæ–­çš„å†…å®¹
                        if len(content) > max_content_length:
                            truncated_content = content[:max_content_length] + "..."
                            print(f"```\n{truncated_content}\n```")
                            print(f"ğŸ’¡ å†…å®¹å·²æˆªæ–­ï¼Œå®Œæ•´å†…å®¹å…± {len(content)} å­—ç¬¦")
                        else:
                            print(f"```\n{content}\n```")
                else:
                    print("âŒ æ–‡æ¡£æ²¡æœ‰å†…å®¹")

            print("=" * 80)

        except Exception as e:
            print(f"âŒ è¯»å–çŸ¥è¯†åº“æ–‡ä»¶å¤±è´¥: {e}")

    async def list_knowledge_base_ids(self) -> List[str]:
        """è¿”å›æ‰€æœ‰çŸ¥è¯†åº“çš„IDåˆ—è¡¨"""
        index_data = await self._load_kb_index()
        return [kb["kb_id"] for kb in index_data["knowledge_bases"]]


if __name__ == "__main__":
    print("loading embeddings...")
    embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-small-zh", model_kwargs={"local_files_only": True})
    print("embeddings loaded.")
    print("loading knowledge base manager...")
    kbm = KnowledgeBaseManager(embeddings)
    print("knowledge base manager loaded.")
    print("Knowledge Base Summary:")
    asyncio.run(kbm.print_all_knowledge_bases(show_full_content=True, max_url_num=50))
