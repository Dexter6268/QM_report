import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import json
import datetime

import hashlib
import pickle
from pathlib import Path
import httpx
import time
from typing import List, Optional, Dict, Any, Union, Literal, Annotated, cast, overload
from enum import Enum
from urllib.parse import unquote
from collections import defaultdict, OrderedDict
import logging

from azure.search.documents.models import VectorQuery
from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.vectorstores import InMemoryVectorStore, VectorStore
from langchain_text_splitters import RecursiveCharacterTextSplitter


class KnowledgeBaseManager:
    def __init__(self, embeddings: Embeddings):
        self.embeddings = embeddings
        self.similarity_threshold = 0.75  # ç›¸ä¼¼åº¦é˜ˆå€¼
        self.project_root = Path(__file__).parent.parent.parent
        self.kb_dir = self.project_root / "knowledge_base"
        self.kb_dir.mkdir(exist_ok=True)
        self.index_file = self.kb_dir / "kb_index.json"

    def _load_kb_index(self) -> Dict[str, Any]:
        """åŠ è½½çŸ¥è¯†åº“ç´¢å¼•"""
        if self.index_file.exists():
            with open(self.index_file, "r", encoding="utf-8") as f:
                return json.load(f)
        return {"knowledge_bases": []}

    def _save_kb_index(self, index_data: Dict[str, Any]) -> None:
        """ä¿å­˜çŸ¥è¯†åº“ç´¢å¼•"""
        with open(self.index_file, "w", encoding="utf-8") as f:
            json.dump(index_data, f, ensure_ascii=False, indent=2)

    def _compute_query_embedding(self, query: str) -> List[float]:
        """è®¡ç®—æŸ¥è¯¢çš„å‘é‡è¡¨ç¤º"""
        return self.embeddings.embed_query(query)

    def find_best_knowledge_base(self, query: str) -> Optional[tuple[str, float]]:
        """æ‰¾åˆ°æœ€åŒ¹é…çš„çŸ¥è¯†åº“"""
        index_data = self._load_kb_index()

        if not index_data["knowledge_bases"]:
            return None

        query_embedding = np.array([self._compute_query_embedding(query)])

        best_kb_id = ""
        best_similarity = 0.0

        for kb_info in index_data["knowledge_bases"]:
            # è®¡ç®—ä¸ŽçŸ¥è¯†åº“ä»£è¡¨æ€§æŸ¥è¯¢çš„ç›¸ä¼¼åº¦
            kb_embedding = np.array([kb_info["query_embedding"]])
            similarity: float = cosine_similarity(query_embedding, kb_embedding)[0][0]

            if similarity > best_similarity:
                best_similarity = similarity
                best_kb_id: str = kb_info["kb_id"]

        if best_similarity >= self.similarity_threshold:
            return best_kb_id, best_similarity

        return None

    def create_knowledge_base(self, query: str) -> str:
        """åˆ›å»ºæ–°çš„çŸ¥è¯†åº“"""
        kb_id = hashlib.md5(f"{query}_{datetime.datetime.now().isoformat()}".encode()).hexdigest()[
            :12
        ]
        query_embedding = self._compute_query_embedding(query)

        index_data = self._load_kb_index()
        index_data["knowledge_bases"].append(
            {
                "kb_id": kb_id,
                "representative_query": query,
                "query_embedding": query_embedding,
                "created_at": datetime.datetime.now().isoformat(),
                "query_count": 1,
                "doc_count": 0,
                "related_queries": [query],
            }
        )

        self._save_kb_index(index_data)
        return kb_id

    def update_knowledge_base_info(self, kb_id: str, query: str, doc_count: int):
        """æ›´æ–°çŸ¥è¯†åº“ä¿¡æ¯"""
        index_data = self._load_kb_index()

        for kb_info in index_data["knowledge_bases"]:
            if kb_info["kb_id"] == kb_id:
                kb_info["query_count"] += 1
                kb_info["doc_count"] = doc_count
                kb_info["related_queries"].append(query)
                kb_info["last_updated"] = datetime.datetime.now().isoformat()

                # å¦‚æžœæŸ¥è¯¢æ•°é‡è¶…è¿‡é˜ˆå€¼ï¼Œé‡æ–°è®¡ç®—ä»£è¡¨æ€§å‘é‡
                if kb_info["query_count"] % 5 == 0:
                    all_queries = kb_info["related_queries"]
                    all_embeddings = [self._compute_query_embedding(q) for q in all_queries]
                    # ä½¿ç”¨å¹³å‡å‘é‡ä½œä¸ºä»£è¡¨æ€§å‘é‡
                    kb_info["query_embedding"] = np.mean(all_embeddings, axis=0).tolist()

                break

        self._save_kb_index(index_data)

    def get_knowledge_base_path(self, kb_id: str) -> Path:
        """èŽ·å–çŸ¥è¯†åº“æ–‡ä»¶è·¯å¾„"""
        return self.kb_dir / f"kb_{kb_id}.pkl"

    def save_knowledge_base(
        self, kb_id: str, vector_store: VectorStore, query: str, documents: List[Document]
    ):
        """ä¿å­˜çŸ¥è¯†åº“"""
        kb_path = self.get_knowledge_base_path(kb_id)

        save_data = {
            "vector_store": vector_store,
            "documents": documents,
            "kb_id": kb_id,
            "last_query": query,
            "updated_at": datetime.datetime.now().isoformat(),
            "doc_count": len(documents),
        }

        try:
            with open(kb_path, "wb") as f:
                pickle.dump(save_data, f)

            # æ›´æ–°ç´¢å¼•ä¿¡æ¯
            self.update_knowledge_base_info(kb_id, query, len(documents))

            logging.info(f"Knowledge base {kb_id} saved with {len(documents)} documents")
        except Exception as e:
            logging.error(f"Failed to save knowledge base {kb_id}: {e}")

    def load_or_create_knowledge_base(
        self, query: str
    ) -> tuple[Optional[VectorStore], List[Document], str, bool]:
        """
        åŠ è½½æˆ–åˆ›å»ºçŸ¥è¯†åº“ - ä¿®æ­£ç‰ˆæœ¬

        Returns:
            Tuple[vector_store, documents, kb_id, is_new]
        """
        # å°è¯•æ‰¾åˆ°æœ€åŒ¹é…çš„çŸ¥è¯†åº“
        match_result = self.find_best_knowledge_base(query)

        if match_result:
            kb_id, similarity = match_result
            logging.info(f"Found matching knowledge base {kb_id} with similarity {similarity:.3f}")

            # åŠ è½½çŽ°æœ‰çŸ¥è¯†åº“
            kb_path = self.get_knowledge_base_path(kb_id)
            if kb_path.exists():
                try:
                    with open(kb_path, "rb") as f:
                        save_data = pickle.load(f)

                    vector_store = save_data["vector_store"]
                    documents = save_data["documents"]

                    logging.info(
                        f"Loaded existing knowledge base {kb_id} with {len(documents)} documents"
                    )
                    return (
                        vector_store,
                        documents,
                        kb_id,
                        False,
                    )  # is_new = Falseï¼Œè¡¨ç¤ºä½¿ç”¨çŽ°æœ‰çŸ¥è¯†åº“
                except Exception as e:
                    logging.error(f"Failed to load knowledge base {kb_id}: {e}")

        # åˆ›å»ºæ–°çŸ¥è¯†åº“
        kb_id = self.create_knowledge_base(query)
        logging.info(f"No Matching results, created new knowledge base {kb_id} for query: {query}")
        return None, [], kb_id, True  # is_new = Trueï¼Œè¡¨ç¤ºæ–°å»ºçŸ¥è¯†åº“

    def print_all_knowledge_bases(
        self, show_full_content: bool = False, max_content_length: int = 200, max_url_num: int = 3
    ) -> None:
        """
        æ‰“å°æ‰€æœ‰çŸ¥è¯†åº“çš„è¯¦ç»†ä¿¡æ¯

        Args:
            show_full_content (bool): æ˜¯å¦æ˜¾ç¤ºå®Œæ•´çš„æ–‡æ¡£å†…å®¹
            max_content_length (int): å½“show_full_content=Falseæ—¶ï¼Œæ¯ä¸ªæ–‡æ¡£æ˜¾ç¤ºçš„æœ€å¤§å­—ç¬¦æ•°
        """
        index_data = self._load_kb_index()

        if not index_data["knowledge_bases"]:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•çŸ¥è¯†åº“")
            return

        print("=" * 100)
        print(f"ðŸ“š çŸ¥è¯†åº“æ€»è§ˆ ({len(index_data['knowledge_bases'])} ä¸ªçŸ¥è¯†åº“)")
        print("=" * 100)

        for i, kb_info in enumerate(index_data["knowledge_bases"], 1):
            kb_id = kb_info["kb_id"]

            print(f"\nðŸ›ï¸  çŸ¥è¯†åº“ {i}: {kb_id}")
            print("-" * 80)
            print(f"ðŸ“ ä»£è¡¨æ€§æŸ¥è¯¢: {kb_info['representative_query']}")
            print(f"ðŸ“… åˆ›å»ºæ—¶é—´: {kb_info['created_at']}")
            print(f"ðŸ”„ æŸ¥è¯¢æ¬¡æ•°: {kb_info['query_count']}")
            print(f"ðŸ“„ æ–‡æ¡£æ•°é‡: {kb_info['doc_count']}")
            print(f"ðŸ•’ æœ€åŽæ›´æ–°: {kb_info.get('last_updated', 'æœªæ›´æ–°')}")

            # æ˜¾ç¤ºç›¸å…³æŸ¥è¯¢
            if len(kb_info["related_queries"]) > 1:
                print(f"ðŸ”— ç›¸å…³æŸ¥è¯¢ ({len(kb_info['related_queries'])}):")
                for j, query in enumerate(kb_info["related_queries"][:5], 1):  # æœ€å¤šæ˜¾ç¤º5ä¸ª
                    print(f"  {j}. {query}")
                if len(kb_info["related_queries"]) > 5:
                    print(f"  ... è¿˜æœ‰ {len(kb_info['related_queries']) - 5} ä¸ªæŸ¥è¯¢")

            # å°è¯•åŠ è½½å¹¶æ˜¾ç¤ºæ–‡æ¡£å†…å®¹
            kb_path = self.get_knowledge_base_path(kb_id)
            if kb_path.exists():
                try:
                    with open(kb_path, "rb") as f:
                        save_data = pickle.load(f)

                    documents = save_data.get("documents", [])
                    if documents:
                        print(f"\nðŸ“š æ–‡æ¡£å†…å®¹ ({len(documents)} ä¸ªæ–‡æ¡£):")

                        # æŒ‰URLåˆ†ç»„æ˜¾ç¤ºæ–‡æ¡£
                        url_groups = defaultdict(list)
                        for doc in documents:
                            url = doc.metadata.get("url", "æœªçŸ¥URL")
                            url_groups[url].append(doc)

                        for i, (url, docs) in enumerate(list(url_groups.items())[:max_url_num], 1):
                            print(f"\nðŸ“š æ–‡æ¡£ {i}\n" + "-" * 80)
                            print(f"\nðŸŒ æ¥æº: {url}")
                            print(f"ðŸ“‘ æ ‡é¢˜: {docs[0].metadata.get('title', 'æœªçŸ¥æ ‡é¢˜')}")
                            print(f"ðŸ“Š æ–‡æ¡£ç‰‡æ®µæ•°: {len(docs)}")

                            # æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ–‡æ¡£ç‰‡æ®µçš„å†…å®¹
                            if docs:
                                content = docs[0].page_content
                                if show_full_content:
                                    print(f"ðŸ“„ å†…å®¹:\n```\n{content}\n```")
                                else:
                                    if len(content) > max_content_length:
                                        truncated_content = content[:max_content_length] + "..."
                                        print(f"ðŸ“„ å†…å®¹é¢„è§ˆ:\n```\n{truncated_content}\n```")
                                    else:
                                        print(f"ðŸ“„ å†…å®¹:\n```\n{content}\n```")

                        if len(url_groups) > max_url_num:
                            print(f"\nðŸ’¡ è¿˜æœ‰ {len(url_groups) - max_url_num} ä¸ªå…¶ä»–æ¥æºçš„æ–‡æ¡£...")
                    else:
                        print("\nâŒ çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ–‡æ¡£å†…å®¹")

                except Exception as e:
                    print(f"\nâŒ æ— æ³•åŠ è½½çŸ¥è¯†åº“æ–‡ä»¶: {e}")
            else:
                print(f"\nâŒ çŸ¥è¯†åº“æ–‡ä»¶ä¸å­˜åœ¨: {kb_path}")

            print("\n" + "=" * 100)

    def print_knowledge_base_summary(self) -> None:
        """æ‰“å°çŸ¥è¯†åº“çš„ç®€è¦ç»Ÿè®¡ä¿¡æ¯"""
        index_data = self._load_kb_index()

        if not index_data["knowledge_bases"]:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•çŸ¥è¯†åº“")
            return

        total_kbs = len(index_data["knowledge_bases"])
        total_docs = sum(kb["doc_count"] for kb in index_data["knowledge_bases"])
        total_queries = sum(kb["query_count"] for kb in index_data["knowledge_bases"])

        print("ðŸ“Š çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯")
        print("=" * 50)
        print(f"ðŸ›ï¸  æ€»çŸ¥è¯†åº“æ•°é‡: {total_kbs}")
        print(f"ðŸ“„ æ€»æ–‡æ¡£æ•°é‡: {total_docs}")
        print(f"ðŸ” æ€»æŸ¥è¯¢æ¬¡æ•°: {total_queries}")
        print(f"ðŸ“ˆ å¹³å‡æ¯ä¸ªçŸ¥è¯†åº“æ–‡æ¡£æ•°: {total_docs / total_kbs:.1f}")
        print(f"ðŸ”„ å¹³å‡æ¯ä¸ªçŸ¥è¯†åº“æŸ¥è¯¢æ¬¡æ•°: {total_queries / total_kbs:.1f}")

        # æ˜¾ç¤ºæœ€æ´»è·ƒçš„çŸ¥è¯†åº“
        most_used_kb = max(index_data["knowledge_bases"], key=lambda x: x["query_count"])
        largest_kb = max(index_data["knowledge_bases"], key=lambda x: x["doc_count"])

        print(f"\nðŸ† æœ€æ´»è·ƒçŸ¥è¯†åº“:")
        print(f"   æŸ¥è¯¢: {most_used_kb['representative_query']}")
        print(f"   ä½¿ç”¨æ¬¡æ•°: {most_used_kb['query_count']}")

        print(f"\nðŸ“š æœ€å¤§çŸ¥è¯†åº“:")
        print(f"   æŸ¥è¯¢: {largest_kb['representative_query']}")
        print(f"   æ–‡æ¡£æ•°é‡: {largest_kb['doc_count']}")

    def inspect_knowledge_base(
        self, kb_id: str, show_full_content: bool = True, max_content_length: int = 500
    ) -> None:
        """
        æŸ¥çœ‹ç‰¹å®šçŸ¥è¯†åº“çš„è¯¦ç»†ä¿¡æ¯

        Args:
            kb_id (str): çŸ¥è¯†åº“ID
            show_full_content (bool): æ˜¯å¦æ˜¾ç¤ºå®Œæ•´çš„æ–‡æ¡£å†…å®¹
            max_content_length (int): å½“show_full_content=Falseæ—¶ï¼Œæ¯ä¸ªæ–‡æ¡£æ˜¾ç¤ºçš„æœ€å¤§å­—ç¬¦æ•°
        """
        # å…ˆä»Žç´¢å¼•ä¸­æŸ¥æ‰¾çŸ¥è¯†åº“ä¿¡æ¯
        index_data = self._load_kb_index()
        kb_info = None

        for kb in index_data["knowledge_bases"]:
            if kb["kb_id"] == kb_id:
                kb_info = kb
                break

        if not kb_info:
            print(f"âŒ æœªæ‰¾åˆ°çŸ¥è¯†åº“ ID: {kb_id}")
            return

        kb_path = self.get_knowledge_base_path(kb_id)

        if not kb_path.exists():
            print(f"âŒ çŸ¥è¯†åº“æ–‡ä»¶ä¸å­˜åœ¨: {kb_path}")
            return

        try:
            with open(kb_path, "rb") as f:
                save_data = pickle.load(f)

            print("=" * 80)
            print(f"ðŸ“š çŸ¥è¯†åº“è¯¦ç»†ä¿¡æ¯: {kb_id}")
            print("=" * 80)
            print(f"ðŸ“ ä»£è¡¨æ€§æŸ¥è¯¢: {kb_info['representative_query']}")
            print(f"ðŸ“… åˆ›å»ºæ—¶é—´: {kb_info['created_at']}")
            print(f"ðŸ”„ æŸ¥è¯¢æ¬¡æ•°: {kb_info['query_count']}")
            print(
                f"ðŸ“„ æ–‡æ¡£æ•°é‡: {save_data.get('doc_count', len(save_data.get('documents', [])))}"
            )
            print(f"ðŸ•’ æœ€åŽæ›´æ–°: {save_data.get('updated_at', 'æœªçŸ¥')}")
            print(f"ðŸ“ æ–‡ä»¶è·¯å¾„: {kb_path}")
            print(f"ðŸ’¾ æ–‡ä»¶å¤§å°: {kb_path.stat().st_size / 1024:.2f} KB")

            # æ˜¾ç¤ºæ‰€æœ‰ç›¸å…³æŸ¥è¯¢
            print(f"\nðŸ”— æ‰€æœ‰ç›¸å…³æŸ¥è¯¢ ({len(kb_info['related_queries'])}):")
            for i, query in enumerate(kb_info["related_queries"], 1):
                print(f"  {i:2d}. {query}")

            documents = save_data.get("documents", [])
            if not documents:
                print("\nâŒ çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ–‡æ¡£")
                return

            print(f"\nðŸ“„ æ–‡æ¡£è¯¦æƒ… ({len(documents)} ä¸ªæ–‡æ¡£):")
            print("=" * 80)

            for i, doc in enumerate(documents, 1):
                print(f"\nðŸ“‘ æ–‡æ¡£ {i}:")
                print("-" * 60)

                # æ˜¾ç¤ºå…ƒæ•°æ®
                if hasattr(doc, "metadata") and doc.metadata:
                    print("ðŸ“‹ å…ƒæ•°æ®:")
                    for key, value in doc.metadata.items():
                        print(f"  {key}: {value}")

                # æ˜¾ç¤ºæ–‡æ¡£å†…å®¹
                if hasattr(doc, "page_content"):
                    content = doc.page_content
                    print(f"\nðŸ“ å†…å®¹ (é•¿åº¦: {len(content)} å­—ç¬¦):")

                    if show_full_content:
                        print(f"```\n{content}\n```")
                    else:
                        # æ˜¾ç¤ºæˆªæ–­çš„å†…å®¹
                        if len(content) > max_content_length:
                            truncated_content = content[:max_content_length] + "..."
                            print(f"```\n{truncated_content}\n```")
                            print(f"ðŸ’¡ å†…å®¹å·²æˆªæ–­ï¼Œå®Œæ•´å†…å®¹å…± {len(content)} å­—ç¬¦")
                        else:
                            print(f"```\n{content}\n```")
                else:
                    print("âŒ æ–‡æ¡£æ²¡æœ‰å†…å®¹")

            print("=" * 80)

        except Exception as e:
            print(f"âŒ è¯»å–çŸ¥è¯†åº“æ–‡ä»¶å¤±è´¥: {e}")

    def list_knowledge_base_ids(self) -> List[str]:
        """è¿”å›žæ‰€æœ‰çŸ¥è¯†åº“çš„IDåˆ—è¡¨"""
        index_data = self._load_kb_index()
        return [kb["kb_id"] for kb in index_data["knowledge_bases"]]


if __name__ == "__main__":
    embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-small-zh")
    kbm = KnowledgeBaseManager(embeddings)
    kbm.print_all_knowledge_bases(show_full_content=True, max_url_num=50)
