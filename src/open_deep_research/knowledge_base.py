import os
import json
import time
import asyncio
import logging
import datetime
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity


import hashlib
import pickle
from pathlib import Path
from typing import List, Optional, Dict, Any
from collections import defaultdict


from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.vectorstores import VectorStore


class KnowledgeBaseManager:
    def __init__(self, embeddings: Embeddings):
        self.embeddings = embeddings
        self.similarity_threshold = 0.75  # ç›¸ä¼¼åº¦é˜ˆå€¼
        self.project_root = Path(__file__).parent.parent.parent
        self.kb_dir = self.project_root / "knowledge_base"
        # self.kb_dir.mkdir(exist_ok=True, parents=True)
        self.index_file = self.kb_dir / "kb_index.json"

    async def _load_kb_index(self) -> Dict[str, Any]:
        """åŠ è½½çŸ¥è¯†åº“ç´¢å¼•"""
        if self.index_file.exists():

            def read_json():
                with open(self.index_file, "r", encoding="utf-8") as f:
                    return json.load(f)

            return await asyncio.to_thread(read_json)
        return {"knowledge_bases": []}

    async def _save_kb_index(self, index_data: Dict[str, Any]) -> None:
        """ä¿å­˜çŸ¥è¯†åº“ç´¢å¼•"""

        def write_json():
            # ç”Ÿæˆå”¯ä¸€çš„ä¸´æ—¶æ–‡ä»¶å
            timestamp = int(time.time() * 1000000)
            process_id = os.getpid()
            temp_file = self.index_file.with_suffix(f".tmp.{process_id}.{timestamp}")

            try:
                # å†™å…¥ä¸´æ—¶æ–‡ä»¶
                with open(temp_file, "w", encoding="utf-8") as f:
                    json.dump(index_data, f, ensure_ascii=False, indent=2)

                # åŽŸå­æ€§æ›¿æ¢
                temp_file.replace(self.index_file)

            except Exception as e:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if temp_file.exists():
                    temp_file.unlink()
                raise e

        await asyncio.to_thread(write_json)

    def _compute_query_embedding(self, query: str) -> List[float]:
        """è®¡ç®—æŸ¥è¯¢çš„å‘é‡è¡¨ç¤º"""
        return self.embeddings.embed_query(query)

    async def find_best_knowledge_base(self, query: str) -> Optional[tuple[str, float]]:
        """æ‰¾åˆ°æœ€åŒ¹é…çš„çŸ¥è¯†åº“"""
        index_data = await self._load_kb_index()

        if not index_data["knowledge_bases"]:
            return None

        query_embedding = np.array([self._compute_query_embedding(query)])

        best_kb_id = ""
        best_similarity = 0.0

        for kb_info in index_data["knowledge_bases"]:
            # è®¡ç®—ä¸ŽçŸ¥è¯†åº“ä»£è¡¨æ€§æŸ¥è¯¢çš„ç›¸ä¼¼åº¦
            kb_embedding = np.array([kb_info["query_embedding"]])
            similarity: float = cosine_similarity(query_embedding, kb_embedding)[0][0]

            if similarity > best_similarity:
                best_similarity = similarity
                best_kb_id: str = kb_info["kb_id"]

        if best_similarity >= self.similarity_threshold:
            return best_kb_id, best_similarity

        return None

    async def create_knowledge_base(self, query: str) -> str:
        """åˆ›å»ºæ–°çš„çŸ¥è¯†åº“"""
        kb_id = hashlib.md5(f"{query}_{datetime.datetime.now().isoformat()}".encode()).hexdigest()[:12]
        query_embedding = self._compute_query_embedding(query)

        index_data = await self._load_kb_index()
        index_data["knowledge_bases"].append(
            {
                "kb_id": kb_id,
                "representative_query": query,
                "query_embedding": query_embedding,
                "created_at": datetime.datetime.now().isoformat(),
                "query_count": 1,
                "doc_count": 0,
                "related_queries": [query],
            }
        )

        await self._save_kb_index(index_data)
        return kb_id

    async def update_knowledge_base_info(self, kb_id: str, query: str, doc_count: int):
        """æ›´æ–°çŸ¥è¯†åº“ä¿¡æ¯"""
        index_data = await self._load_kb_index()

        for kb_info in index_data["knowledge_bases"]:
            if kb_info["kb_id"] == kb_id:
                kb_info["query_count"] += 1
                kb_info["doc_count"] = doc_count
                kb_info["related_queries"].append(query)
                kb_info["last_updated"] = datetime.datetime.now().isoformat()

                # å¦‚æžœæŸ¥è¯¢æ•°é‡è¶…è¿‡é˜ˆå€¼ï¼Œé‡æ–°è®¡ç®—ä»£è¡¨æ€§å‘é‡
                if kb_info["query_count"] % 5 == 0:
                    all_queries = kb_info["related_queries"]
                    all_embeddings = [self._compute_query_embedding(q) for q in all_queries]
                    # ä½¿ç”¨å¹³å‡å‘é‡ä½œä¸ºä»£è¡¨æ€§å‘é‡
                    kb_info["query_embedding"] = np.mean(all_embeddings, axis=0).tolist()

                break

        await self._save_kb_index(index_data)

    def get_knowledge_base_path(self, kb_id: str) -> Path:
        """èŽ·å–çŸ¥è¯†åº“æ–‡ä»¶è·¯å¾„"""
        return self.kb_dir / f"kb_{kb_id}.pkl"

    async def save_knowledge_base(self, kb_id: str, vector_store: VectorStore, query: str, documents: List[Document]):
        """ä¿å­˜çŸ¥è¯†åº“"""

        def write_pickle():
            timestamp = int(time.time() * 1000000)
            process_id = os.getpid()
            kb_path = self.get_knowledge_base_path(kb_id)
            temp_path = kb_path.with_suffix(f".tmp.{process_id}.{timestamp}")

            save_data = {
                "vector_store": vector_store,
                "documents": documents,
                "kb_id": kb_id,
                "last_query": query,
                "updated_at": datetime.datetime.now().isoformat(),
                "doc_count": len(documents),
            }

            try:
                with open(temp_path, "wb") as f:
                    pickle.dump(save_data, f)

                # åŽŸå­æ€§æ›¿æ¢
                temp_path.replace(kb_path)

            except Exception as e:
                if temp_path.exists():
                    temp_path.unlink()
                raise e

        try:
            await asyncio.to_thread(write_pickle)
            await self.update_knowledge_base_info(kb_id, query, len(documents))
            logging.info(f"Knowledge base {kb_id} saved with {len(documents)} documents")
        except Exception as e:
            logging.error(f"Failed to save knowledge base {kb_id}: {e}")

    async def load_or_create_knowledge_base(
        self, query: str
    ) -> tuple[Optional[VectorStore], List[Document], str, bool]:
        """
        åŠ è½½æˆ–åˆ›å»ºçŸ¥è¯†åº“

        Returns:
            Tuple[vector_store, documents, kb_id, is_new]
        """
        # å°è¯•æ‰¾åˆ°æœ€åŒ¹é…çš„çŸ¥è¯†åº“
        match_result = await self.find_best_knowledge_base(query)

        if match_result:
            kb_id, similarity = match_result
            logging.info(f"Found matching knowledge base {kb_id} with similarity {similarity:.3f}")

            # åŠ è½½çŽ°æœ‰çŸ¥è¯†åº“
            kb_path = self.get_knowledge_base_path(kb_id)
            if kb_path.exists():
                try:
                    save_data = await asyncio.to_thread(lambda: pickle.load(open(kb_path, "rb")))

                    vector_store = save_data["vector_store"]
                    documents = save_data["documents"]

                    logging.info(f"Loaded existing knowledge base {kb_id} with {len(documents)} documents")
                    return (
                        vector_store,
                        documents,
                        kb_id,
                        False,
                    )  # is_new = Falseï¼Œè¡¨ç¤ºä½¿ç”¨çŽ°æœ‰çŸ¥è¯†åº“
                except Exception as e:
                    logging.error(f"Failed to load knowledge base {kb_id}: {e}")

        # åˆ›å»ºæ–°çŸ¥è¯†åº“
        kb_id = await self.create_knowledge_base(query)
        logging.info(f"No Matching results, created new knowledge base {kb_id} for query: {query}")
        return None, [], kb_id, True  # is_new = Trueï¼Œè¡¨ç¤ºæ–°å»ºçŸ¥è¯†åº“

    async def print_all_knowledge_bases(
        self, show_full_content: bool = False, max_content_length: int = 200, max_url_num: int = 3
    ) -> None:
        """
        æ‰“å°æ‰€æœ‰çŸ¥è¯†åº“çš„è¯¦ç»†ä¿¡æ¯

        Args:
            show_full_content (bool): æ˜¯å¦æ˜¾ç¤ºå®Œæ•´çš„æ–‡æ¡£å†…å®¹
            max_content_length (int): å½“show_full_content=Falseæ—¶ï¼Œæ¯ä¸ªæ–‡æ¡£æ˜¾ç¤ºçš„æœ€å¤§å­—ç¬¦æ•°
        """
        index_data = await self._load_kb_index()

        if not index_data["knowledge_bases"]:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•çŸ¥è¯†åº“")
            return

        print("=" * 100)
        print(f"ðŸ“š çŸ¥è¯†åº“æ€»è§ˆ ({len(index_data['knowledge_bases'])} ä¸ªçŸ¥è¯†åº“)")
        print("=" * 100)

        for i, kb_info in enumerate(index_data["knowledge_bases"], 1):
            kb_id = kb_info["kb_id"]

            print(f"\nðŸ›ï¸  çŸ¥è¯†åº“ {i}: {kb_id}")
            print("-" * 80)
            print(f"ðŸ“ ä»£è¡¨æ€§æŸ¥è¯¢: {kb_info['representative_query']}")
            print(f"ðŸ“… åˆ›å»ºæ—¶é—´: {kb_info['created_at']}")
            print(f"ðŸ”„ æŸ¥è¯¢æ¬¡æ•°: {kb_info['query_count']}")
            print(f"ðŸ“„ æ–‡æ¡£æ•°é‡: {kb_info['doc_count']}")
            print(f"ðŸ•’ æœ€åŽæ›´æ–°: {kb_info.get('last_updated', 'æœªæ›´æ–°')}")

            # æ˜¾ç¤ºç›¸å…³æŸ¥è¯¢
            if len(kb_info["related_queries"]) > 1:
                print(f"ðŸ”— ç›¸å…³æŸ¥è¯¢ ({len(kb_info['related_queries'])}):")
                for j, query in enumerate(kb_info["related_queries"][:5], 1):  # æœ€å¤šæ˜¾ç¤º5ä¸ª
                    print(f"  {j}. {query}")
                if len(kb_info["related_queries"]) > 5:
                    print(f"  ... è¿˜æœ‰ {len(kb_info['related_queries']) - 5} ä¸ªæŸ¥è¯¢")

            # å°è¯•åŠ è½½å¹¶æ˜¾ç¤ºæ–‡æ¡£å†…å®¹
            kb_path = self.get_knowledge_base_path(kb_id)
            if kb_path.exists():
                try:
                    with open(kb_path, "rb") as f:
                        save_data = pickle.load(f)

                    documents = save_data.get("documents", [])
                    if documents:
                        print(f"\nðŸ“š æ–‡æ¡£å†…å®¹ ({len(documents)} ä¸ªæ–‡æ¡£):")

                        # æŒ‰URLåˆ†ç»„æ˜¾ç¤ºæ–‡æ¡£
                        url_groups = defaultdict(list)
                        for doc in documents:
                            url = doc.metadata.get("url", "æœªçŸ¥URL")
                            url_groups[url].append(doc)

                        for i, (url, docs) in enumerate(list(url_groups.items())[:max_url_num], 1):
                            print(f"\nðŸ“š æ–‡æ¡£ {i}\n" + "-" * 80)
                            print(f"\nðŸŒ æ¥æº: {url}")
                            print(f"ðŸ“‘ æ ‡é¢˜: {docs[0].metadata.get('title', 'æœªçŸ¥æ ‡é¢˜')}")
                            print(f"ðŸ“Š æ–‡æ¡£ç‰‡æ®µæ•°: {len(docs)}")

                            # æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ–‡æ¡£ç‰‡æ®µçš„å†…å®¹
                            if docs:
                                content = docs[0].page_content
                                if show_full_content:
                                    print(f"ðŸ“„ å†…å®¹:\n```\n{content}\n```")
                                else:
                                    if len(content) > max_content_length:
                                        truncated_content = content[:max_content_length] + "..."
                                        print(f"ðŸ“„ å†…å®¹é¢„è§ˆ:\n```\n{truncated_content}\n```")
                                    else:
                                        print(f"ðŸ“„ å†…å®¹:\n```\n{content}\n```")

                        if len(url_groups) > max_url_num:
                            print(f"\nðŸ’¡ è¿˜æœ‰ {len(url_groups) - max_url_num} ä¸ªå…¶ä»–æ¥æºçš„æ–‡æ¡£...")
                    else:
                        print("\nâŒ çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ–‡æ¡£å†…å®¹")

                except Exception as e:
                    print(f"\nâŒ æ— æ³•åŠ è½½çŸ¥è¯†åº“æ–‡ä»¶: {e}")
            else:
                print(f"\nâŒ çŸ¥è¯†åº“æ–‡ä»¶ä¸å­˜åœ¨: {kb_path}")

            print("\n" + "=" * 100)

    async def print_knowledge_base_summary(self) -> None:
        """æ‰“å°çŸ¥è¯†åº“çš„ç®€è¦ç»Ÿè®¡ä¿¡æ¯"""
        index_data = await self._load_kb_index()

        if not index_data["knowledge_bases"]:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•çŸ¥è¯†åº“")
            return

        total_kbs = len(index_data["knowledge_bases"])
        total_docs = sum(kb["doc_count"] for kb in index_data["knowledge_bases"])
        total_queries = sum(kb["query_count"] for kb in index_data["knowledge_bases"])

        print("ðŸ“Š çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯")
        print("=" * 50)
        print(f"ðŸ›ï¸  æ€»çŸ¥è¯†åº“æ•°é‡: {total_kbs}")
        print(f"ðŸ“„ æ€»æ–‡æ¡£æ•°é‡: {total_docs}")
        print(f"ðŸ” æ€»æŸ¥è¯¢æ¬¡æ•°: {total_queries}")
        print(f"ðŸ“ˆ å¹³å‡æ¯ä¸ªçŸ¥è¯†åº“æ–‡æ¡£æ•°: {total_docs / total_kbs:.1f}")
        print(f"ðŸ”„ å¹³å‡æ¯ä¸ªçŸ¥è¯†åº“æŸ¥è¯¢æ¬¡æ•°: {total_queries / total_kbs:.1f}")

        # æ˜¾ç¤ºæœ€æ´»è·ƒçš„çŸ¥è¯†åº“
        most_used_kb = max(index_data["knowledge_bases"], key=lambda x: x["query_count"])
        largest_kb = max(index_data["knowledge_bases"], key=lambda x: x["doc_count"])

        print(f"\nðŸ† æœ€æ´»è·ƒçŸ¥è¯†åº“:")
        print(f"   æŸ¥è¯¢: {most_used_kb['representative_query']}")
        print(f"   ä½¿ç”¨æ¬¡æ•°: {most_used_kb['query_count']}")

        print(f"\nðŸ“š æœ€å¤§çŸ¥è¯†åº“:")
        print(f"   æŸ¥è¯¢: {largest_kb['representative_query']}")
        print(f"   æ–‡æ¡£æ•°é‡: {largest_kb['doc_count']}")

    async def inspect_knowledge_base(
        self, kb_id: str, show_full_content: bool = True, max_content_length: int = 500
    ) -> None:
        """
        æŸ¥çœ‹ç‰¹å®šçŸ¥è¯†åº“çš„è¯¦ç»†ä¿¡æ¯

        Args:
            kb_id (str): çŸ¥è¯†åº“ID
            show_full_content (bool): æ˜¯å¦æ˜¾ç¤ºå®Œæ•´çš„æ–‡æ¡£å†…å®¹
            max_content_length (int): å½“show_full_content=Falseæ—¶ï¼Œæ¯ä¸ªæ–‡æ¡£æ˜¾ç¤ºçš„æœ€å¤§å­—ç¬¦æ•°
        """
        # å…ˆä»Žç´¢å¼•ä¸­æŸ¥æ‰¾çŸ¥è¯†åº“ä¿¡æ¯
        index_data = await self._load_kb_index()
        kb_info = None

        for kb in index_data["knowledge_bases"]:
            if kb["kb_id"] == kb_id:
                kb_info = kb
                break

        if not kb_info:
            print(f"âŒ æœªæ‰¾åˆ°çŸ¥è¯†åº“ ID: {kb_id}")
            return

        kb_path = self.get_knowledge_base_path(kb_id)

        if not kb_path.exists():
            print(f"âŒ çŸ¥è¯†åº“æ–‡ä»¶ä¸å­˜åœ¨: {kb_path}")
            return

        try:
            with open(kb_path, "rb") as f:
                save_data = pickle.load(f)

            print("=" * 80)
            print(f"ðŸ“š çŸ¥è¯†åº“è¯¦ç»†ä¿¡æ¯: {kb_id}")
            print("=" * 80)
            print(f"ðŸ“ ä»£è¡¨æ€§æŸ¥è¯¢: {kb_info['representative_query']}")
            print(f"ðŸ“… åˆ›å»ºæ—¶é—´: {kb_info['created_at']}")
            print(f"ðŸ”„ æŸ¥è¯¢æ¬¡æ•°: {kb_info['query_count']}")
            print(f"ðŸ“„ æ–‡æ¡£æ•°é‡: {save_data.get('doc_count', len(save_data.get('documents', [])))}")
            print(f"ðŸ•’ æœ€åŽæ›´æ–°: {save_data.get('updated_at', 'æœªçŸ¥')}")
            print(f"ðŸ“ æ–‡ä»¶è·¯å¾„: {kb_path}")
            print(f"ðŸ’¾ æ–‡ä»¶å¤§å°: {kb_path.stat().st_size / 1024:.2f} KB")

            # æ˜¾ç¤ºæ‰€æœ‰ç›¸å…³æŸ¥è¯¢
            print(f"\nðŸ”— æ‰€æœ‰ç›¸å…³æŸ¥è¯¢ ({len(kb_info['related_queries'])}):")
            for i, query in enumerate(kb_info["related_queries"], 1):
                print(f"  {i:2d}. {query}")

            documents = save_data.get("documents", [])
            if not documents:
                print("\nâŒ çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ–‡æ¡£")
                return

            print(f"\nðŸ“„ æ–‡æ¡£è¯¦æƒ… ({len(documents)} ä¸ªæ–‡æ¡£):")
            print("=" * 80)

            for i, doc in enumerate(documents, 1):
                print(f"\nðŸ“‘ æ–‡æ¡£ {i}:")
                print("-" * 60)

                # æ˜¾ç¤ºå…ƒæ•°æ®
                if hasattr(doc, "metadata") and doc.metadata:
                    print("ðŸ“‹ å…ƒæ•°æ®:")
                    for key, value in doc.metadata.items():
                        print(f"  {key}: {value}")

                # æ˜¾ç¤ºæ–‡æ¡£å†…å®¹
                if hasattr(doc, "page_content"):
                    content = doc.page_content
                    print(f"\nðŸ“ å†…å®¹ (é•¿åº¦: {len(content)} å­—ç¬¦):")

                    if show_full_content:
                        print(f"```\n{content}\n```")
                    else:
                        # æ˜¾ç¤ºæˆªæ–­çš„å†…å®¹
                        if len(content) > max_content_length:
                            truncated_content = content[:max_content_length] + "..."
                            print(f"```\n{truncated_content}\n```")
                            print(f"ðŸ’¡ å†…å®¹å·²æˆªæ–­ï¼Œå®Œæ•´å†…å®¹å…± {len(content)} å­—ç¬¦")
                        else:
                            print(f"```\n{content}\n```")
                else:
                    print("âŒ æ–‡æ¡£æ²¡æœ‰å†…å®¹")

            print("=" * 80)

        except Exception as e:
            print(f"âŒ è¯»å–çŸ¥è¯†åº“æ–‡ä»¶å¤±è´¥: {e}")

    async def list_knowledge_base_ids(self) -> List[str]:
        """è¿”å›žæ‰€æœ‰çŸ¥è¯†åº“çš„IDåˆ—è¡¨"""
        index_data = await self._load_kb_index()
        return [kb["kb_id"] for kb in index_data["knowledge_bases"]]


if __name__ == "__main__":
    embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-small-zh")
    kbm = KnowledgeBaseManager(embeddings)
    asyncio.run(kbm.print_all_knowledge_bases(show_full_content=True, max_url_num=50))
